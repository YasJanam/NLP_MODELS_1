## Labratory_10
یک سری پارامتر های مدل پوشه 9 را تغییر دادیم و یک مدل تعریف کردیم.سپس آن مدل را 4 بار train کردیم.

توجه داشته باشید در این کد __یک مدل__ را 4 بار train کردیم نه این که 4 مدل را train کردیم. ( کلا یک بار در کد مدل تعریف کردیم و در چهار train آن را آموزش دادیم )

به این صورت که مدل train شده را دوباره train کردیم ( مدل حاصل از train_1  را دوباره train کردیم. دوباره مدل حاصله از train_2 را دوباره train کردیم. و دوباره مدل حاصله از train_3 را دوباره train کردیم. یعنی مدل را 4 بار پشت سر هم train کردیم. )

در کل مدل را روی 100000 داده train کردیم که البته برای تسک خلاصه سازی چیزی نیست و در نتایج هم میبینیم که در کل مدل به نتیجه بدرد بخوری نرسیده ! 

پارامتر های مدل بارت این کد در زیر قابل مشاهده است :

__🔶model_params : 🔸d_model=64 , 🔸n_heads=2 , 🔸d_ff=512 , 🔸num_enc=2 , 🔸num_dec=2 , 🔸max_len=128__


### train_results

| train_num | train_size | val_size | epuch_num | val_loss | val_rouge1 | val_rouge2 | val_rougeL | val_rougeLsum |
|:-----:|:------:|:---:|:------:|:----:|:-----:|:------:|:--------:|:---------------------:|
| 1 | 20000 | 4000 | 1 | 6.220 | 0.195 | 0.0158 | 0.153 | 0.186 |
| 2 | 30000 | 5000 | 2 | 6.120 | 0.218 | 0.0182 | 0.166 | 0.204 |
| 3 | 20000 | 4000 | 5 | 6.510 | 0.228 | 0.0177  | 0.164 | 0.212 |  
| 4 | 30000  | 3000 | 3 | 5.650 | 0.244 | 0.0215 | 0.178 | 0.223 |

### تحلیل نتایج
مقادیر rouge نسبتاً پایین هستن (معمولاً برای یک مدل خوب روی دیتاست استاندارد، ROUGE-1 بالای 0.3 یا 0.4، ROUGE-2 بالای 0.1 و ROUGE-L بالای 0.25 دیده میشه)

پایین بودن ROUGE-2 مخصوصاً نشون میده که مدل توی حفظ ساختار و توالی درست کلمات خوب عمل نکرده.

امکان داره مدل بیشتر به بازنویسی آزاد یا پارافرایز تمایل داشته باشه تا بازتولید دقیق.

نتایج تست مدل نشان میده که مدل حتی جملات بامفهوم تولید نمیکنه، فقط توالی کلمات را تا حدودی یاد گرفته و برای تسکی مثل خلاصه سازی توانایی ندارد



---
### دلایل نامطلوب بودن نتایج ارزیابی مدل پس از 4 آموزش
1. __نبود pretraining__ : مدل های بارت برای تسک هایی مثل خلاصه سازی اول روی داده بسیار زیادی پیش آموزش میشوند.سپس برای تسک مربوطه ( مثلا خلاصه سازی ، ترجمه و ...) آموزش میبینند.اما ما در این کد چیزی به نام پیش آموزش نداشتیم و مستقیم رفتیم سراغ آموزش مدل !!

 برای پیش آموزش مدل های بارت از Denoising Autoencoding استفاده میکنند.
   
2. __مدل کوچک__ : مدل های بارت برای تسک خلاصه سازی حداقل 6×6 هستند، ولی این مدل 2×2 است

بهتر است مدل حداقل 4×4 باشد.
 
3. __کم بودن d_model و max_len__ : کم بودن این پارامتر ها باعث میشود مدل عبارات طولانی و ارتباط معنایی بین توکن ها را یاد نگیرد یا دیر یاد بگیرد.
 
 پیشنهاد : d_model = 256 و max_len = 512

 وفتی d_model کم باشد شبکه ظرفیت ندارد که وابستگی های طولانی بین جمله ها رو یادبگیرد

### 🔹 فرق اصلی پیش‌آموزش با آموزش مستقیم

1. هدف (Objective)

   * پیش‌آموزش معمولاً با هدف‌های عمومی انجام میشه مثل:

     * ***Masked Language Modeling (MLM)*** → حدس زدن توکن‌های حذف‌شده
     * ***Denoising Autoencoding*** → بازسازی متن ناقص یا بهم‌ریخته
   * ولی آموزش روی CNN/DailyMail فقط هدفش خلاصه‌سازی هست.
   * پیش‌آموزش باعث میشه مدل «زبان» رو یاد بگیرد، بعداً راحت‌تر یاد می‌گیره «خلاصه‌سازی» کنه.

2. داده (Corpus)

   * پیش‌آموزش روی دیتاست‌های خیلی خیلی بزرگ و عمومی انجام میشود (کتاب‌ها، ویکی‌پدیا، وب).
   * دیتای CNN/DailyMail کوچیک‌تر است و فقط خبر و خلاصه دارد -> تنوع کمی داره.
   * بدون اون دیتای عظیم، مدل دایره‌ی لغت و دستور زبان کافی پیدا نمی‌کند.

3. دانش انتقالی (Transfer)

   * وقتی مدل روی دیتای عمومی زبان رو یاد گرفت، موقع فاین‌تیون کردن روی دیتای کوچیک‌تر (مثل خلاصه‌سازی خبر)، فقط لازمه «مهارت خاص» رو تمرین کنه.
   * اگه مستقیم روی خبرها آموزش بدیم، باید همزمان هم «زبان یاد بگیره» هم «خلاصه‌سازی»، که معمولاً برای مدل کوچیک نشدنیه.


