## Labratory_10
یک سری پارامتر های مدل پوشه 9 را تغییر دادیم و یک مدل تعریف کردیم.سپس آن مدل را 4 بار train کردیم.

توجه داشته باشید در این کد __یک مدل__ را 4 بار train کردیم نه این که 4 مدل را train کردیم.

در کل مدل را روی 100000 داده train کردیم که البته برای تسک خلاصه سازی چیزی نیست و در نتایج هم میبینیم که در کل مدل به نتیجه بدرد بخوری نرسیده ! 

پارامتر های مدل بارت این کد در زیر قابل مشاهده است :

__🔶model_params : 🔸d_model=64 , 🔸n_heads=2 , 🔸d_ff=512 , 🔸num_enc=2 , 🔸num_dec=2 , 🔸max_len=128__


### train_results

| train_num | train_size | val_size | epuch_num | val_loss | val_rouge1 | val_rouge2 | val_rougeL | val_rougeLsum |
|:-----:|:------:|:---:|:------:|:----:|:-----:|:------:|:--------:|:---------------------:|
| 1 | 20000 | 4000 | 1 | 6.220 | 0.195 | 0.0158 | 0.153 | 0.186 |
| 2 | 30000 | 5000 | 2 | 6.120 | 0.218 | 0.0182 | 0.166 | 0.204 |
| 3 | 20000 | 4000 | 5 | 6.510 | 0.228 | 0.0177  | 0.164 | 0.212 |  
| 4 | 30000  | 3000 | 3 | 5.650 | 0.244 | 0.0215 | 0.178 | 0.223 |

### تحلیل نتایج
مقادیر rouge نسبتاً پایین هستن (معمولاً برای یک مدل خوب روی دیتاست استاندارد، ROUGE-1 بالای 0.3 یا 0.4، ROUGE-2 بالای 0.1 و ROUGE-L بالای 0.25 دیده میشه)

پایین بودن ROUGE-2 مخصوصاً نشون میده که مدل توی حفظ ساختار و توالی درست کلمات خوب عمل نکرده.

امکان داره مدل بیشتر به بازنویسی آزاد یا پارافرایز تمایل داشته باشه تا بازتولید دقیق.



---
### دلایل نامطلوب بودن نتایج حاصل از 4 آموزش
1. نبود pretrain : مدل های بارت برای تسک هایی مثل خلاصه سازی اول روی داده بسیار زیادی پیش آموزش میشوند.سپس برای تسک مربوطه ( مثلا خلاصه سازی ، ترجمه و ...) آموزش میبینند.اما ما در این کد چیزی به نام پیش آموزش نداشتیم و مستقیم رفتیم سراغ آموزش مدل !!
2. مدل کوچک : مدل های بارت برای تسک خلاصه سازی حداقل 6×6 هستند، ولی این مدل 2×2 است
3. کم بودن d_model و max_len : کم بودن این پارامتر ها باعث میشود مدل عبارات طولانی و ارتباط معنایی بین توکن ها را یاد نگیرد یا دیر یاد بگیرد.
