{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6EOdKoMhlUjo"
      ],
      "authorship_tag": "ABX9TyPVR1R0vHIL7qHeQQZWsgD3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasJanam/NLP_MODELS_1/blob/main/Pretrain_Methods_11/Pretrain_Methods_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PRETRAIN**"
      ],
      "metadata": {
        "id": "TWBD-P54GUpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🌞پیش آموزش مدل های بارت یکی از مهم ترین قسمت های اموزش این مدل هاست\n",
        "پیش آموزش مثل این است که مدل اول زبان را یادبگیرد.سپس برای وظیفه مربوطه،مثلا خلاصه سازی یا ترجمه آموزش ببیند\n",
        "\n",
        "🌟 پیش‌آموزش معمولاً با هدف‌های عمومی انجام میشه مثل :\n",
        "\n",
        " - Masked Language Modeling (MLM) → حدس زدن توکن‌های حذف‌ شده\n",
        "\n",
        " - Denoising Autoencoding → بازسازی متن ناقص یا بهم‌ ریخته"
      ],
      "metadata": {
        "id": "kagW-_tE60Zc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "skRYnA_zkIwg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "پیش آموزش مدل های بارت در حقیقت خودش یک آموزش است که روی یک سری دیتاست ها انجام میشود.در حقیقت چیزی که باید برای پیش آموزش آماده کنیم مدل خاص،یا ابزار آموزش خاصی نیست؛بلکه باید دیتاست مناسب پیش آموزش را آماده کنیم\n",
        "\n",
        "🟧 __دیتاست مناسب برای پیش آموزش چه دیتاستی است ؟__\n",
        "\n",
        "دیتاست مناسب برای پیش آموزش یک دیتاست متنی است ، به این صورت که هر نمونه از این دیتاست دو متن اصلی دارد : یکی متنی به هم ریخته و نویز دار و دیگری مرتب شده همان متن به هم ریخته و نویز دار\n",
        "\n",
        "🟡 __شکل کلی دیتاست مناسب پیش آموزش :__    \n",
        "\n",
        " - __text__ ⟹ متن به هم ریخته،نویز دار همراه با جای خالی\n",
        "\n",
        " - __label__ ⟹ شکل صحیح و درست همان متن نویز دار\n",
        "\n",
        "\n",
        "در مرحله پیش آموزش،مدل یاد میگیرد که متن به هم ریخته را درست کند\n",
        "\n",
        "✅ __چیزهایی که مدل در مرحله پیش آموزش یاد میگیرد__\n",
        "\n",
        "1. جمله بندی و ساختار کلی جمله ها\n",
        "2. واژگان و چگونگی استفاده از آنها در جمله\n",
        "3. توانایی مرتب کردن جمله به هم ریخته\n",
        "4. توانایی پر کردن جای خالی در جمله\n",
        "5. و در کل آشنایی با زبان مربوطه و قواعدش\n",
        "\n",
        "\n",
        "پس میبینیم که این مرحله چقدر ضروری بوده است. بدون پیش آموزش مثل این است که بچه ای که صفر هست و زبان بلد نیست را یک دفعه ای ببریم و بهش خلاصه سازی یاد بدهیم\n",
        "\n",
        "🔴 پس چیزی که بسیار مهم است آماده سازی یک دیتاست خوب است"
      ],
      "metadata": {
        "id": "JdbtxTBzXp7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "برای ساخت دیتاست مناسب پیش آموزش نیاز به یک دیتاست متنی داریم که فقط متن داشته باشد ⟹ **\"wikitext-103-v1\"**\n",
        "\n",
        "🟩 دیتاستی که میخواهیم بسازیم در کل سه ستون دارد :    \n",
        "1. input_ids ⟹ توکنایز شده متن بهم ریخته\n",
        "2. attention_mask ⟹ ماسک متن بهم ریخته\n",
        "3. labels ⟹ توکنایز شده متن اصلی و سالم\n",
        "\n",
        "🟥 مسیر کلی برای ساخت این دیتاست :    \n",
        "1. تعریف یک سری متد برای به هم ریختن یک جمله، ایجاد جای خالی در جمله به شکل تصادفی و نویز دادن به جمله\n",
        "2. استفاده از متد های بالا روی همه جملات دیتاست\n",
        "3. توکنایز کردن جمله های بدست آمده از مرحله دو\n",
        "4. توکنایز کردن فرم اصلی هر جمله و قرار دادن آن کنار توکنایز شده فرم به هم ریخته اش\n",
        "\n",
        "↪ در ادامه متد های تعریف شده و شیوه به کارگیری آنها برای ساخت دیتاست دیده میشود"
      ],
      "metadata": {
        "id": "DOjkcnakdTYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "PAD_ID = tokenizer.pad_token_id\n",
        "MASK_ID =  tokenizer.mask_token_id\n",
        "EOS_ID = tokenizer.eos_token_id\n",
        "BOS_ID = tokenizer.bos_token_id"
      ],
      "metadata": {
        "id": "MvBwtaIwlrSV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Artificial Intelligence (AI) is a branch of computer science that focuses on building systems capable of performing tasks that normally require human intelligence.\n",
        " These tasks include learning, reasoning, problem-solving, and understanding natural language.\n",
        "  AI technologies are widely used today, from recommendation systems and chatbots to self-driving cars and medical diagnosis.\n",
        " As AI continues to develop, it raises both exciting opportunities and important ethical challenges for society.\"\"\""
      ],
      "metadata": {
        "id": "12WWPpq0gbeK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **methods**"
      ],
      "metadata": {
        "id": "IuweYomLw9zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# حذف خطوط خالی\n",
        "def clean_lines(lines):\n",
        "  out = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if len(line) > 0:\n",
        "      out.append(line)\n",
        "  return out"
      ],
      "metadata": {
        "id": "KLhgab9dJtcG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_txt = clean_lines(train_txt)\n",
        "val_txt = clean_lines(val_txt)"
      ],
      "metadata": {
        "id": "V8-mB9yyJ59-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **split_sentences**\n",
        "این متد یک متن را میگیرد و در عوض لیستی از جمله های آن متد را میدهد. در حقیقت متن را جمله جمله میکند\n",
        "\n",
        "در ادامه مثالش را روی تسک بالا میبینیم"
      ],
      "metadata": {
        "id": "XDocMdqQgok7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ابزار جمله بندی خیلی ساده\n",
        "def split_sentences(x):\n",
        "  out = []\n",
        "  s = []\n",
        "  for tok in x.split():\n",
        "    s.append(tok)\n",
        "    if tok.endswith(('.','!','?','.\"',\"!'\",\"?'\")):\n",
        "      out.append(\" \".join(s))\n",
        "      s = []\n",
        "  if s:\n",
        "    out.append(\" \".join(s))\n",
        "  return out if out else [x]"
      ],
      "metadata": {
        "id": "GJlwXYB8KBlQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "text_sncs = split_sentences(text)\n",
        "text_sncs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdlWUkavhM9Y",
        "outputId": "f4164a62-0d83-44ae-cc78-63efff7104c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial Intelligence (AI) is a branch of computer science that focuses on building systems capable of performing tasks that normally require human intelligence.',\n",
              " 'These tasks include learning, reasoning, problem-solving, and understanding natural language.',\n",
              " 'AI technologies are widely used today, from recommendation systems and chatbots to self-driving cars and medical diagnosis.',\n",
              " 'As AI continues to develop, it raises both exciting opportunities and important ethical challenges for society.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **simple_span_len**\n",
        "این متد تنها کاری که میکند این است که یک طول به ما بر میگرداند.این طول تصادفی است و از توزیع پواسون استفاده میشود\n",
        "\n",
        "اما این طول ها به چه دردی میخورند ؟\n",
        "\n",
        "از این طول ها وقتی استفاده میکنیم که بخواهیم یک توالی در یک جمله را جای خالی کنیم. طول این توالی برای جای خالی کردن به کمک این متد بدست می آید، در ادامه خواهید دید\n"
      ],
      "metadata": {
        "id": "8NLmrfk-kK2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# نمونه گیری طول با توزیع پواسون\n",
        "def simple_span_len(lam=3):\n",
        "  L=0\n",
        "  p = math.exp(-lam)\n",
        "  F = p\n",
        "  u = random.random()\n",
        "  while u > F:\n",
        "    L += 1\n",
        "    p *= lam/L\n",
        "    F += p\n",
        "  return max(L,1)"
      ],
      "metadata": {
        "id": "0lt-JuJ5LAYl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = simple_span_len()\n",
        "l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTWD3w4zkBt_",
        "outputId": "609cf873-151d-4197-b8b2-cbb19daa8b77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **text_infilling_token_ids**\n",
        "این متد برای جای خالی کرد یک توالی در یک جمله است\n",
        "\n",
        "جمله ها کار میشود ids در این متد با فرم\n",
        "\n",
        "توضیح کد :    \n",
        "\n",
        "محاسبه یک طول تصادفی :    \n",
        "\n",
        "    L = len(ids)\n",
        "\n",
        "محاسبه طول دقیق طول توالی برای ماسک کردن :\n",
        "\n",
        "(که باید جای خالی شوند ids  تعداد عدد هایی از )     \n",
        "\n",
        "    num_to_mask = max(1,int(mask_ratio*L))\n",
        "\n",
        "تعریف لیست ماسک شده، لیستی که در نهایت برگردانده میشود.در ابتدا این لیست   برابر جمله اصلی است:\n",
        "\n",
        "    masked = ids[:]\n",
        "\n",
        "تعریف مجموعه ای که شامل ایندکس هایی از جمله است که ماسک شده اند\n",
        "\n",
        "    covered = set()\n",
        "\n",
        "شروع فرایند ماسک کردن :    \n",
        "      \n",
        "    while len(covered) < num_to_mask:\n",
        "      start = random.randrange(0,L)\n",
        "      if start in covered:\n",
        "        continue\n",
        "      span_len = simple_span_len(lam)\n",
        "      end = min(L,start+span_len)\n",
        "      for i in range(start,end):\n",
        "        if i not in covered:\n",
        "          masked[i] = MASK_ID\n",
        "          covered.add(i)\n",
        "\n",
        "---\n",
        "شرح فرآیند ماسک کردن :\n",
        "( while  حلقه )   \n",
        "\n",
        "فرایند ماسک کردن را به اندازه طول محاسبه شده برای توالی ماسک انجام بده:\n",
        "\n",
        "    while len(covered) < num_to_mask:\n",
        "\n",
        "محاسبه ایندکس تصادفی برای شروع ماسک گذاری:\n",
        "\n",
        "      start = random.randrange(0,L)\n",
        "\n",
        "این خط میگوید اگرایندکس شروع ماسک گذاری قبلا ماسک شده است برو و دوباره ایندکس شروع را محاسبه کن:\n",
        "\n",
        "      if start in covered:\n",
        "        continue\n",
        "\n",
        "محاسبه ایندکس پایان ماسک گذاری\n",
        "( توجه کنید که ایندکس پایان ماسک گذاری لزوما ماسک شدن به اندازه محاسبه شده را تضمین نمیکند، برای همین در حلقه شروع فرایند ماسک گذاری، تعداد ماسک شده ها رو بررسی میکنیم.)\n",
        "\n",
        "      span_len = simple_span_len(lam)\n",
        "      end = min(L,start+span_len)\n",
        "\n",
        "حالا از ایندکس شرو تا ایندکس پایان شروع به ماسک کردن میکند، هر چه که ماسک شد  اضافه میشود covered ایندکسش به مجموعه\n",
        "\n",
        "      for i in range(start,end):\n",
        "        if i not in covered:\n",
        "          masked[i] = MASK_ID\n",
        "          covered.add(i)"
      ],
      "metadata": {
        "id": "6EOdKoMhlUjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_infilling_token_ids(ids,mask_ratio=0.3,lam=3):\n",
        "  L = len(ids)\n",
        "  num_to_mask = max(1,int(mask_ratio*L))\n",
        "  masked = ids[:]\n",
        "  covered = set()\n",
        "  while len(covered) < num_to_mask:\n",
        "    start = random.randrange(0,L)\n",
        "    if start in covered:\n",
        "      continue\n",
        "    span_len = simple_span_len(lam)\n",
        "    end = min(L,start+span_len)\n",
        "    for i in range(start,end):\n",
        "      if i not in covered:\n",
        "        masked[i] = MASK_ID\n",
        "        covered.add(i)\n",
        "  return masked"
      ],
      "metadata": {
        "id": "nobYXry2LnX5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "masked_txt =  text_infilling_token_ids(tokenizer(text)[\"input_ids\"])\n",
        "print(tokenizer.decode(masked_txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIOIdlM7s4G7",
        "outputId": "072b680c-e9dd-4e3e-e577-d82639651dbb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s><mask>ificial Intelligence (AI) is a branch of computer science that focuses on building systems capable<mask> performing<mask><mask><mask> require human<mask>.<mask><mask><mask> include learning, reasoning, problem-solving, and understanding natural language.\n",
            "  AI technologies are widely used today,<mask> recommendation systems and chatbots to<mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask> raises both exciting opportunities and important ethical challenges for society<mask></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **sentence_permute**\n",
        "\n",
        "به هم ریختن جمله ها"
      ],
      "metadata": {
        "id": "sqvnvT9yt9Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_permute(text):\n",
        "  sents = split_sentences(text)\n",
        "  random.shuffle(sents)\n",
        "  return \" \".join(sents)"
      ],
      "metadata": {
        "id": "WUohZE6zMzhq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "sent_per = sentence_permute(text)\n",
        "sent_per"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "zPW2IGC3u0P9",
        "outputId": "da679749-2fec-4469-aa9f-5dc84d30cc52"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI technologies are widely used today, from recommendation systems and chatbots to self-driving cars and medical diagnosis. Artificial Intelligence (AI) is a branch of computer science that focuses on building systems capable of performing tasks that normally require human intelligence. These tasks include learning, reasoning, problem-solving, and understanding natural language. As AI continues to develop, it raises both exciting opportunities and important ethical challenges for society.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **apply_noise**\n",
        "این متد یک تسکت میگیرد. جمله های تسکت را به هم میریزد. روی تسکت نویز اعمال میکند\n",
        "\n",
        " ست میکند max_len سپس طول متن نویزی را به اندازه حداکثر طول یا همان\n",
        "\n",
        " ماسک توجه هم میسازد که نشان دهد کدام بخش هایی از جمله، واقعا قسمتی از جمله است، و کدام بخش هایی از جمله پدینگ هست\n",
        "\n",
        " در این متد از متد های تعریف شده قبلی استفاده شده است\n",
        "\n",
        " است pack تنها قسمت جدید این متد، متد\n",
        "\n",
        "\n",
        " 🟪 **pack** ⟶\n",
        "\n",
        "مشخص کردن اول و اخر جمله\n",
        "\n",
        "    arr = [BOS_ID] + arr[:max_len-2] + [EOS_ID]\n",
        "\n",
        "ماسک توجه : تاجایی که طول جمله است، 1 بزار\n",
        "\n",
        "    attn = [1]*len(arr)\n",
        "\n",
        "در تکه کد زیر طول جمله را به اندازه طول حداکثری میکنیم.اگر طول جمله کم بود، به ان اضافه میکنیم (padding ):    \n",
        "\n",
        "    if len(arr) < max_len:\n",
        "      pad_len = max_len - len(arr)\n",
        "      arr += [PAD_ID]*pad_len\n",
        "      attn += [0]*pad_len\n",
        "\n",
        "ماسک توجه برای قسمت های پد شده برابر صفر است:\n",
        "\n",
        "    attn += [0]*pad_len"
      ],
      "metadata": {
        "id": "dOtm0Cj7xhn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INPUT = 512\n",
        "MAX_TARGET = 128\n",
        "\n",
        "def apply_noise(text,do_sentperm=True,mask_ratio = 0.3,lam=3):\n",
        "\n",
        "  # میتوان تعیین کرد که جمله ها به هم بریزند یا نه\n",
        "  if do_sentperm:\n",
        "    text = sentence_permute(text)\n",
        "\n",
        "  # ids -> این مربوط به جمله اصلی است.یعنی شکل صحیح و درست جمله\n",
        "  ids = tokenizer(text,truncation=True,max_length=MAX_INPUT,add_special_tokens=False)[\"input_ids\"]\n",
        "  if len(ids) == 0:\n",
        "    return None\n",
        "\n",
        "  noisy_ids = text_infilling_token_ids(ids,mask_ratio=mask_ratio,lam=lam)\n",
        "\n",
        "\n",
        "  def pack(arr,max_len):\n",
        "    arr = [BOS_ID] + arr[:max_len-2] + [EOS_ID]\n",
        "    attn = [1]*len(arr)\n",
        "    if len(arr) < max_len:\n",
        "      pad_len = max_len - len(arr)\n",
        "      arr += [PAD_ID]*pad_len\n",
        "      attn += [0]*pad_len\n",
        "    return arr, attn\n",
        "\n",
        "  noisy_imp, noisy_attn = pack(noisy_ids,MAX_INPUT)\n",
        "  clean_tgt, _  = pack(ids,MAX_TARGET)\n",
        "\n",
        "  return {\n",
        "      \"input_ids\":noisy_imp, # شکل نویزی جمله\n",
        "      \"attention_mask\":noisy_attn, # ماسک توجه شکل نویزی جمله\n",
        "      \"labels\":clean_tgt  # شکل صحیح جمله\n",
        "  }"
      ],
      "metadata": {
        "id": "c2rcYKj_M7Bm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nois_txt = apply_noise(text)\n",
        "print(tokenizer.decode(nois_txt[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhzEkmx64J4G",
        "outputId": "a7f9b4ae-20df-4cb6-91f4-696f58e6f61e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>Artificial Intelligence (<mask><mask> is a branch of computer science that<mask><mask><mask><mask><mask><mask><mask> tasks that normally<mask><mask> intelligence<mask><mask><mask><mask><mask><mask> reasoning<mask><mask>-solving, and understanding<mask><mask>.<mask><mask><mask> to develop, it raises both exciting opportunities and important ethical challenges for society. AI technologies are widely used today, from recommendation systems and chatbots to self-driving cars and medical diagnosis<mask></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **make_tensor_dataset**\n",
        "این متد دیتاست را میگیرد و نویز را به تکست های دیتاست اعمال میکند.خروجی را به بر میگرداند Dataset فرم"
      ],
      "metadata": {
        "id": "-7KC4ASA3j1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tensor_dataset(texts, n_limits=None):\n",
        "  rows = []\n",
        "  count = 0\n",
        "  for t in texts:\n",
        "    ex = apply_noise(t,do_sentperm=True,mask_ratio=0.3,lam=3)\n",
        "    if ex is not None:\n",
        "      rows.append(ex)\n",
        "      count += 1\n",
        "      if n_limits is not None and count >= n_limits:\n",
        "          break\n",
        "\n",
        "  input_ids = torch.tensor([r[\"input_ids\"] for r in rows], dtype=torch.long)\n",
        "  attention_mask = torch.tensor([r[\"attention_mask\"] for r in rows], dtype=torch.long)\n",
        "  labels = torch.tensor([r[\"labels\"] for r in rows], dtype=torch.long)\n",
        "  return {\"input_ids\":input_ids,\"attention_mask\":attention_mask, \"labels\":labels}"
      ],
      "metadata": {
        "id": "vhwWoNUvQtYI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = make_tensor_dataset(text)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMkaeEdQ4oEd",
        "outputId": "d01bb6c0-0058-4120-96c3-482e05ca5841"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         ...,\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'labels': tensor([[  0, 250,   2,  ...,   1,   1,   1],\n",
              "         [  0, 338,   2,  ...,   1,   1,   1],\n",
              "         [  0,  90,   2,  ...,   1,   1,   1],\n",
              "         ...,\n",
              "         [  0,  90,   2,  ...,   1,   1,   1],\n",
              "         [  0, 219,   2,  ...,   1,   1,   1],\n",
              "         [  0,   4,   2,  ...,   1,   1,   1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}