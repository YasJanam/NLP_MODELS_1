{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6EOdKoMhlUjo"
      ],
      "authorship_tag": "ABX9TyPVR1R0vHIL7qHeQQZWsgD3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasJanam/NLP_MODELS_1/blob/main/Pretrain_Methods_11/Pretrain_Methods_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PRETRAIN**"
      ],
      "metadata": {
        "id": "TWBD-P54GUpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸŒÙ¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù‡Ø§ÛŒ Ø¨Ø§Ø±Øª ÛŒÚ©ÛŒ Ø§Ø² Ù…Ù‡Ù… ØªØ±ÛŒÙ† Ù‚Ø³Ù…Øª Ù‡Ø§ÛŒ Ø§Ù…ÙˆØ²Ø´ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù‡Ø§Ø³Øª\n",
        "Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø«Ù„ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…Ø¯Ù„ Ø§ÙˆÙ„ Ø²Ø¨Ø§Ù† Ø±Ø§ ÛŒØ§Ø¯Ø¨Ú¯ÛŒØ±Ø¯.Ø³Ù¾Ø³ Ø¨Ø±Ø§ÛŒ ÙˆØ¸ÛŒÙÙ‡ Ù…Ø±Ø¨ÙˆØ·Ù‡ØŒÙ…Ø«Ù„Ø§ Ø®Ù„Ø§ØµÙ‡ Ø³Ø§Ø²ÛŒ ÛŒØ§ ØªØ±Ø¬Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø¨ÛŒÙ†Ø¯\n",
        "\n",
        "ğŸŒŸ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø§ Ù‡Ø¯Ùâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ´Ù‡ Ù…Ø«Ù„ :\n",
        "\n",
        " - Masked Language Modeling (MLM) â†’ Ø­Ø¯Ø³ Ø²Ø¯Ù† ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ùâ€Œ Ø´Ø¯Ù‡\n",
        "\n",
        " - Denoising Autoencoding â†’ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù…ØªÙ† Ù†Ø§Ù‚Øµ ÛŒØ§ Ø¨Ù‡Ù…â€Œ Ø±ÛŒØ®ØªÙ‡"
      ],
      "metadata": {
        "id": "kagW-_tE60Zc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "skRYnA_zkIwg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù‡Ø§ÛŒ Ø¨Ø§Ø±Øª Ø¯Ø± Ø­Ù‚ÛŒÙ‚Øª Ø®ÙˆØ¯Ø´ ÛŒÚ© Ø¢Ù…ÙˆØ²Ø´ Ø§Ø³Øª Ú©Ù‡ Ø±ÙˆÛŒ ÛŒÚ© Ø³Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ù‡Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ´ÙˆØ¯.Ø¯Ø± Ø­Ù‚ÛŒÙ‚Øª Ú†ÛŒØ²ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ… Ù…Ø¯Ù„ Ø®Ø§ØµØŒÛŒØ§ Ø§Ø¨Ø²Ø§Ø± Ø¢Ù…ÙˆØ²Ø´ Ø®Ø§ØµÛŒ Ù†ÛŒØ³ØªØ›Ø¨Ù„Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¯ÛŒØªØ§Ø³Øª Ù…Ù†Ø§Ø³Ø¨ Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ø±Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ…\n",
        "\n",
        "ğŸŸ§ __Ø¯ÛŒØªØ§Ø³Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ú†Ù‡ Ø¯ÛŒØªØ§Ø³ØªÛŒ Ø§Ø³Øª ØŸ__\n",
        "\n",
        "Ø¯ÛŒØªØ§Ø³Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© Ø¯ÛŒØªØ§Ø³Øª Ù…ØªÙ†ÛŒ Ø§Ø³Øª ØŒ Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª Ú©Ù‡ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¯Ùˆ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø¯Ø§Ø±Ø¯ : ÛŒÚ©ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ùˆ Ù†ÙˆÛŒØ² Ø¯Ø§Ø± Ùˆ Ø¯ÛŒÚ¯Ø±ÛŒ Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡ Ù‡Ù…Ø§Ù† Ù…ØªÙ† Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ùˆ Ù†ÙˆÛŒØ² Ø¯Ø§Ø±\n",
        "\n",
        "ğŸŸ¡ __Ø´Ú©Ù„ Ú©Ù„ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ù…Ù†Ø§Ø³Ø¨ Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ :__    \n",
        "\n",
        " - __text__ âŸ¹ Ù…ØªÙ† Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ØŒÙ†ÙˆÛŒØ² Ø¯Ø§Ø± Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ\n",
        "\n",
        " - __label__ âŸ¹ Ø´Ú©Ù„ ØµØ­ÛŒØ­ Ùˆ Ø¯Ø±Ø³Øª Ù‡Ù…Ø§Ù† Ù…ØªÙ† Ù†ÙˆÛŒØ² Ø¯Ø§Ø±\n",
        "\n",
        "\n",
        "Ø¯Ø± Ù…Ø±Ø­Ù„Ù‡ Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ØŒÙ…Ø¯Ù„ ÛŒØ§Ø¯ Ù…ÛŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡ Ù…ØªÙ† Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ø±Ø§ Ø¯Ø±Ø³Øª Ú©Ù†Ø¯\n",
        "\n",
        "âœ… __Ú†ÛŒØ²Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¯Ø± Ù…Ø±Ø­Ù„Ù‡ Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ ÛŒØ§Ø¯ Ù…ÛŒÚ¯ÛŒØ±Ø¯__\n",
        "\n",
        "1. Ø¬Ù…Ù„Ù‡ Ø¨Ù†Ø¯ÛŒ Ùˆ Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ Ø¬Ù…Ù„Ù‡ Ù‡Ø§\n",
        "2. ÙˆØ§Ú˜Ú¯Ø§Ù† Ùˆ Ú†Ú¯ÙˆÙ†Ú¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù†Ù‡Ø§ Ø¯Ø± Ø¬Ù…Ù„Ù‡\n",
        "3. ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ù…Ø±ØªØ¨ Ú©Ø±Ø¯Ù† Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡\n",
        "4. ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø¯Ø± Ø¬Ù…Ù„Ù‡\n",
        "5. Ùˆ Ø¯Ø± Ú©Ù„ Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ Ø²Ø¨Ø§Ù† Ù…Ø±Ø¨ÙˆØ·Ù‡ Ùˆ Ù‚ÙˆØ§Ø¹Ø¯Ø´\n",
        "\n",
        "\n",
        "Ù¾Ø³ Ù…ÛŒØ¨ÛŒÙ†ÛŒÙ… Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ú†Ù‚Ø¯Ø± Ø¶Ø±ÙˆØ±ÛŒ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª. Ø¨Ø¯ÙˆÙ† Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø«Ù„ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¨Ú†Ù‡ Ø§ÛŒ Ú©Ù‡ ØµÙØ± Ù‡Ø³Øª Ùˆ Ø²Ø¨Ø§Ù† Ø¨Ù„Ø¯ Ù†ÛŒØ³Øª Ø±Ø§ ÛŒÚ© Ø¯ÙØ¹Ù‡ Ø§ÛŒ Ø¨Ø¨Ø±ÛŒÙ… Ùˆ Ø¨Ù‡Ø´ Ø®Ù„Ø§ØµÙ‡ Ø³Ø§Ø²ÛŒ ÛŒØ§Ø¯ Ø¨Ø¯Ù‡ÛŒÙ…\n",
        "\n",
        "ğŸ”´ Ù¾Ø³ Ú†ÛŒØ²ÛŒ Ú©Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù… Ø§Ø³Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ ÛŒÚ© Ø¯ÛŒØªØ§Ø³Øª Ø®ÙˆØ¨ Ø§Ø³Øª"
      ],
      "metadata": {
        "id": "JdbtxTBzXp7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø³Øª Ù…Ù†Ø§Ø³Ø¨ Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ ÛŒÚ© Ø¯ÛŒØªØ§Ø³Øª Ù…ØªÙ†ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ú©Ù‡ ÙÙ‚Ø· Ù…ØªÙ† Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ âŸ¹ **\"wikitext-103-v1\"**\n",
        "\n",
        "ğŸŸ© Ø¯ÛŒØªØ§Ø³ØªÛŒ Ú©Ù‡ Ù…ÛŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ø³Ø§Ø²ÛŒÙ… Ø¯Ø± Ú©Ù„ Ø³Ù‡ Ø³ØªÙˆÙ† Ø¯Ø§Ø±Ø¯ :    \n",
        "1. input_ids âŸ¹ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ù…ØªÙ† Ø¨Ù‡Ù… Ø±ÛŒØ®ØªÙ‡\n",
        "2. attention_mask âŸ¹ Ù…Ø§Ø³Ú© Ù…ØªÙ† Ø¨Ù‡Ù… Ø±ÛŒØ®ØªÙ‡\n",
        "3. labels âŸ¹ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ùˆ Ø³Ø§Ù„Ù…\n",
        "\n",
        "ğŸŸ¥ Ù…Ø³ÛŒØ± Ú©Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª :    \n",
        "1. ØªØ¹Ø±ÛŒÙ ÛŒÚ© Ø³Ø±ÛŒ Ù…ØªØ¯ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡ØŒ Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ø´Ú©Ù„ ØªØµØ§Ø¯ÙÛŒ Ùˆ Ù†ÙˆÛŒØ² Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ø¬Ù…Ù„Ù‡\n",
        "2. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØ¯ Ù‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ Ø±ÙˆÛŒ Ù‡Ù…Ù‡ Ø¬Ù…Ù„Ø§Øª Ø¯ÛŒØªØ§Ø³Øª\n",
        "3. ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¬Ù…Ù„Ù‡ Ù‡Ø§ÛŒ Ø¨Ø¯Ø³Øª Ø¢Ù…Ø¯Ù‡ Ø§Ø² Ù…Ø±Ø­Ù„Ù‡ Ø¯Ùˆ\n",
        "4. ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† ÙØ±Ù… Ø§ØµÙ„ÛŒ Ù‡Ø± Ø¬Ù…Ù„Ù‡ Ùˆ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ø¢Ù† Ú©Ù†Ø§Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ ÙØ±Ù… Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ø§Ø´\n",
        "\n",
        "â†ª Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ù…ØªØ¯ Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ùˆ Ø´ÛŒÙˆÙ‡ Ø¨Ù‡ Ú©Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¢Ù†Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø³Øª Ø¯ÛŒØ¯Ù‡ Ù…ÛŒØ´ÙˆØ¯"
      ],
      "metadata": {
        "id": "DOjkcnakdTYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "PAD_ID = tokenizer.pad_token_id\n",
        "MASK_ID =  tokenizer.mask_token_id\n",
        "EOS_ID = tokenizer.eos_token_id\n",
        "BOS_ID = tokenizer.bos_token_id"
      ],
      "metadata": {
        "id": "MvBwtaIwlrSV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Artificial Intelligence (AI) is a branch of computer science that focuses on building systems capable of performing tasks that normally require human intelligence.\n",
        " These tasks include learning, reasoning, problem-solving, and understanding natural language.\n",
        "  AI technologies are widely used today, from recommendation systems and chatbots to self-driving cars and medical diagnosis.\n",
        " As AI continues to develop, it raises both exciting opportunities and important ethical challenges for society.\"\"\""
      ],
      "metadata": {
        "id": "12WWPpq0gbeK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **methods**"
      ],
      "metadata": {
        "id": "IuweYomLw9zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ\n",
        "def clean_lines(lines):\n",
        "  out = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if len(line) > 0:\n",
        "      out.append(line)\n",
        "  return out"
      ],
      "metadata": {
        "id": "KLhgab9dJtcG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_txt = clean_lines(train_txt)\n",
        "val_txt = clean_lines(val_txt)"
      ],
      "metadata": {
        "id": "V8-mB9yyJ59-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **split_sentences**\n",
        "Ø§ÛŒÙ† Ù…ØªØ¯ ÛŒÚ© Ù…ØªÙ† Ø±Ø§ Ù…ÛŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø¯Ø± Ø¹ÙˆØ¶ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ù‡Ø§ÛŒ Ø¢Ù† Ù…ØªØ¯ Ø±Ø§ Ù…ÛŒØ¯Ù‡Ø¯. Ø¯Ø± Ø­Ù‚ÛŒÙ‚Øª Ù…ØªÙ† Ø±Ø§ Ø¬Ù…Ù„Ù‡ Ø¬Ù…Ù„Ù‡ Ù…ÛŒÚ©Ù†Ø¯\n",
        "\n",
        "Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø«Ø§Ù„Ø´ Ø±Ø§ Ø±ÙˆÛŒ ØªØ³Ú© Ø¨Ø§Ù„Ø§ Ù…ÛŒØ¨ÛŒÙ†ÛŒÙ…"
      ],
      "metadata": {
        "id": "XDocMdqQgok7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø§Ø¨Ø²Ø§Ø± Ø¬Ù…Ù„Ù‡ Ø¨Ù†Ø¯ÛŒ Ø®ÛŒÙ„ÛŒ Ø³Ø§Ø¯Ù‡\n",
        "def split_sentences(x):\n",
        "  out = []\n",
        "  s = []\n",
        "  for tok in x.split():\n",
        "    s.append(tok)\n",
        "    if tok.endswith(('.','!','?','.\"',\"!'\",\"?'\")):\n",
        "      out.append(\" \".join(s))\n",
        "      s = []\n",
        "  if s:\n",
        "    out.append(\" \".join(s))\n",
        "  return out if out else [x]"
      ],
      "metadata": {
        "id": "GJlwXYB8KBlQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "text_sncs = split_sentences(text)\n",
        "text_sncs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdlWUkavhM9Y",
        "outputId": "f4164a62-0d83-44ae-cc78-63efff7104c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial Intelligence (AI) is a branch of computer science that focuses on building systems capable of performing tasks that normally require human intelligence.',\n",
              " 'These tasks include learning, reasoning, problem-solving, and understanding natural language.',\n",
              " 'AI technologies are widely used today, from recommendation systems and chatbots to self-driving cars and medical diagnosis.',\n",
              " 'As AI continues to develop, it raises both exciting opportunities and important ethical challenges for society.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **simple_span_len**\n",
        "Ø§ÛŒÙ† Ù…ØªØ¯ ØªÙ†Ù‡Ø§ Ú©Ø§Ø±ÛŒ Ú©Ù‡ Ù…ÛŒÚ©Ù†Ø¯ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ ÛŒÚ© Ø·ÙˆÙ„ Ø¨Ù‡ Ù…Ø§ Ø¨Ø± Ù…ÛŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.Ø§ÛŒÙ† Ø·ÙˆÙ„ ØªØµØ§Ø¯ÙÛŒ Ø§Ø³Øª Ùˆ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù¾ÙˆØ§Ø³ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯\n",
        "\n",
        "Ø§Ù…Ø§ Ø§ÛŒÙ† Ø·ÙˆÙ„ Ù‡Ø§ Ø¨Ù‡ Ú†Ù‡ Ø¯Ø±Ø¯ÛŒ Ù…ÛŒØ®ÙˆØ±Ù†Ø¯ ØŸ\n",
        "\n",
        "Ø§Ø² Ø§ÛŒÙ† Ø·ÙˆÙ„ Ù‡Ø§ ÙˆÙ‚ØªÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ… ÛŒÚ© ØªÙˆØ§Ù„ÛŒ Ø¯Ø± ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ú©Ù†ÛŒÙ…. Ø·ÙˆÙ„ Ø§ÛŒÙ† ØªÙˆØ§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ú©Ù…Ú© Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø¯Ø³Øª Ù…ÛŒ Ø¢ÛŒØ¯ØŒ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ø¯ÛŒØ¯\n"
      ],
      "metadata": {
        "id": "8NLmrfk-kK2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÛŒØ±ÛŒ Ø·ÙˆÙ„ Ø¨Ø§ ØªÙˆØ²ÛŒØ¹ Ù¾ÙˆØ§Ø³ÙˆÙ†\n",
        "def simple_span_len(lam=3):\n",
        "  L=0\n",
        "  p = math.exp(-lam)\n",
        "  F = p\n",
        "  u = random.random()\n",
        "  while u > F:\n",
        "    L += 1\n",
        "    p *= lam/L\n",
        "    F += p\n",
        "  return max(L,1)"
      ],
      "metadata": {
        "id": "0lt-JuJ5LAYl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = simple_span_len()\n",
        "l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTWD3w4zkBt_",
        "outputId": "609cf873-151d-4197-b8b2-cbb19daa8b77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **text_infilling_token_ids**\n",
        "Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø±Ø§ÛŒ Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ú©Ø±Ø¯ ÛŒÚ© ØªÙˆØ§Ù„ÛŒ Ø¯Ø± ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ø§Ø³Øª\n",
        "\n",
        "Ø¬Ù…Ù„Ù‡ Ù‡Ø§ Ú©Ø§Ø± Ù…ÛŒØ´ÙˆØ¯ ids Ø¯Ø± Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø§ ÙØ±Ù…\n",
        "\n",
        "ØªÙˆØ¶ÛŒØ­ Ú©Ø¯ :    \n",
        "\n",
        "Ù…Ø­Ø§Ø³Ø¨Ù‡ ÛŒÚ© Ø·ÙˆÙ„ ØªØµØ§Ø¯ÙÛŒ :    \n",
        "\n",
        "    L = len(ids)\n",
        "\n",
        "Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø·ÙˆÙ„ Ø¯Ù‚ÛŒÙ‚ Ø·ÙˆÙ„ ØªÙˆØ§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø§Ø³Ú© Ú©Ø±Ø¯Ù† :\n",
        "\n",
        "(Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø´ÙˆÙ†Ø¯ ids  ØªØ¹Ø¯Ø§Ø¯ Ø¹Ø¯Ø¯ Ù‡Ø§ÛŒÛŒ Ø§Ø² )     \n",
        "\n",
        "    num_to_mask = max(1,int(mask_ratio*L))\n",
        "\n",
        "ØªØ¹Ø±ÛŒÙ Ù„ÛŒØ³Øª Ù…Ø§Ø³Ú© Ø´Ø¯Ù‡ØŒ Ù„ÛŒØ³ØªÛŒ Ú©Ù‡ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯.Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø§ÛŒÙ† Ù„ÛŒØ³Øª   Ø¨Ø±Ø§Ø¨Ø± Ø¬Ù…Ù„Ù‡ Ø§ØµÙ„ÛŒ Ø§Ø³Øª:\n",
        "\n",
        "    masked = ids[:]\n",
        "\n",
        "ØªØ¹Ø±ÛŒÙ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒ Ú©Ù‡ Ø´Ø§Ù…Ù„ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù…Ø§Ø³Ú© Ø´Ø¯Ù‡ Ø§Ù†Ø¯\n",
        "\n",
        "    covered = set()\n",
        "\n",
        "Ø´Ø±ÙˆØ¹ ÙØ±Ø§ÛŒÙ†Ø¯ Ù…Ø§Ø³Ú© Ú©Ø±Ø¯Ù† :    \n",
        "      \n",
        "    while len(covered) < num_to_mask:\n",
        "      start = random.randrange(0,L)\n",
        "      if start in covered:\n",
        "        continue\n",
        "      span_len = simple_span_len(lam)\n",
        "      end = min(L,start+span_len)\n",
        "      for i in range(start,end):\n",
        "        if i not in covered:\n",
        "          masked[i] = MASK_ID\n",
        "          covered.add(i)\n",
        "\n",
        "---\n",
        "Ø´Ø±Ø­ ÙØ±Ø¢ÛŒÙ†Ø¯ Ù…Ø§Ø³Ú© Ú©Ø±Ø¯Ù† :\n",
        "( while  Ø­Ù„Ù‚Ù‡ )   \n",
        "\n",
        "ÙØ±Ø§ÛŒÙ†Ø¯ Ù…Ø§Ø³Ú© Ú©Ø±Ø¯Ù† Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø·ÙˆÙ„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ù„ÛŒ Ù…Ø§Ø³Ú© Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡:\n",
        "\n",
        "    while len(covered) < num_to_mask:\n",
        "\n",
        "Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ù…Ø§Ø³Ú© Ú¯Ø°Ø§Ø±ÛŒ:\n",
        "\n",
        "      start = random.randrange(0,L)\n",
        "\n",
        "Ø§ÛŒÙ† Ø®Ø· Ù…ÛŒÚ¯ÙˆÛŒØ¯ Ø§Ú¯Ø±Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø´Ø±ÙˆØ¹ Ù…Ø§Ø³Ú© Ú¯Ø°Ø§Ø±ÛŒ Ù‚Ø¨Ù„Ø§ Ù…Ø§Ø³Ú© Ø´Ø¯Ù‡ Ø§Ø³Øª Ø¨Ø±Ùˆ Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø´Ø±ÙˆØ¹ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†:\n",
        "\n",
        "      if start in covered:\n",
        "        continue\n",
        "\n",
        "Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù¾Ø§ÛŒØ§Ù† Ù…Ø§Ø³Ú© Ú¯Ø°Ø§Ø±ÛŒ\n",
        "( ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù¾Ø§ÛŒØ§Ù† Ù…Ø§Ø³Ú© Ú¯Ø°Ø§Ø±ÛŒ Ù„Ø²ÙˆÙ…Ø§ Ù…Ø§Ø³Ú© Ø´Ø¯Ù† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ø±Ø§ ØªØ¶Ù…ÛŒÙ† Ù†Ù…ÛŒÚ©Ù†Ø¯ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…ÛŒÙ† Ø¯Ø± Ø­Ù„Ù‚Ù‡ Ø´Ø±ÙˆØ¹ ÙØ±Ø§ÛŒÙ†Ø¯ Ù…Ø§Ø³Ú© Ú¯Ø°Ø§Ø±ÛŒØŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø§Ø³Ú© Ø´Ø¯Ù‡ Ù‡Ø§ Ø±Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒÚ©Ù†ÛŒÙ….)\n",
        "\n",
        "      span_len = simple_span_len(lam)\n",
        "      end = min(L,start+span_len)\n",
        "\n",
        "Ø­Ø§Ù„Ø§ Ø§Ø² Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø´Ø±Ùˆ ØªØ§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù¾Ø§ÛŒØ§Ù† Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ù…Ø§Ø³Ú© Ú©Ø±Ø¯Ù† Ù…ÛŒÚ©Ù†Ø¯ØŒ Ù‡Ø± Ú†Ù‡ Ú©Ù‡ Ù…Ø§Ø³Ú© Ø´Ø¯  Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒØ´ÙˆØ¯ covered Ø§ÛŒÙ†Ø¯Ú©Ø³Ø´ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡\n",
        "\n",
        "      for i in range(start,end):\n",
        "        if i not in covered:\n",
        "          masked[i] = MASK_ID\n",
        "          covered.add(i)"
      ],
      "metadata": {
        "id": "6EOdKoMhlUjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_infilling_token_ids(ids,mask_ratio=0.3,lam=3):\n",
        "  L = len(ids)\n",
        "  num_to_mask = max(1,int(mask_ratio*L))\n",
        "  masked = ids[:]\n",
        "  covered = set()\n",
        "  while len(covered) < num_to_mask:\n",
        "    start = random.randrange(0,L)\n",
        "    if start in covered:\n",
        "      continue\n",
        "    span_len = simple_span_len(lam)\n",
        "    end = min(L,start+span_len)\n",
        "    for i in range(start,end):\n",
        "      if i not in covered:\n",
        "        masked[i] = MASK_ID\n",
        "        covered.add(i)\n",
        "  return masked"
      ],
      "metadata": {
        "id": "nobYXry2LnX5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "masked_txt =  text_infilling_token_ids(tokenizer(text)[\"input_ids\"])\n",
        "print(tokenizer.decode(masked_txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIOIdlM7s4G7",
        "outputId": "072b680c-e9dd-4e3e-e577-d82639651dbb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s><mask>ificial Intelligence (AI) is a branch of computer science that focuses on building systems capable<mask> performing<mask><mask><mask> require human<mask>.<mask><mask><mask> include learning, reasoning, problem-solving, and understanding natural language.\n",
            "  AI technologies are widely used today,<mask> recommendation systems and chatbots to<mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask> raises both exciting opportunities and important ethical challenges for society<mask></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **sentence_permute**\n",
        "\n",
        "Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ† Ø¬Ù…Ù„Ù‡ Ù‡Ø§"
      ],
      "metadata": {
        "id": "sqvnvT9yt9Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_permute(text):\n",
        "  sents = split_sentences(text)\n",
        "  random.shuffle(sents)\n",
        "  return \" \".join(sents)"
      ],
      "metadata": {
        "id": "WUohZE6zMzhq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "sent_per = sentence_permute(text)\n",
        "sent_per"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "zPW2IGC3u0P9",
        "outputId": "da679749-2fec-4469-aa9f-5dc84d30cc52"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI technologies are widely used today, from recommendation systems and chatbots to self-driving cars and medical diagnosis. Artificial Intelligence (AI) is a branch of computer science that focuses on building systems capable of performing tasks that normally require human intelligence. These tasks include learning, reasoning, problem-solving, and understanding natural language. As AI continues to develop, it raises both exciting opportunities and important ethical challenges for society.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **apply_noise**\n",
        "Ø§ÛŒÙ† Ù…ØªØ¯ ÛŒÚ© ØªØ³Ú©Øª Ù…ÛŒÚ¯ÛŒØ±Ø¯. Ø¬Ù…Ù„Ù‡ Ù‡Ø§ÛŒ ØªØ³Ú©Øª Ø±Ø§ Ø¨Ù‡ Ù‡Ù… Ù…ÛŒØ±ÛŒØ²Ø¯. Ø±ÙˆÛŒ ØªØ³Ú©Øª Ù†ÙˆÛŒØ² Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒÚ©Ù†Ø¯\n",
        "\n",
        " Ø³Øª Ù…ÛŒÚ©Ù†Ø¯ max_len Ø³Ù¾Ø³ Ø·ÙˆÙ„ Ù…ØªÙ† Ù†ÙˆÛŒØ²ÛŒ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ ÛŒØ§ Ù‡Ù…Ø§Ù†\n",
        "\n",
        " Ù…Ø§Ø³Ú© ØªÙˆØ¬Ù‡ Ù‡Ù… Ù…ÛŒØ³Ø§Ø²Ø¯ Ú©Ù‡ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ú©Ø¯Ø§Ù… Ø¨Ø®Ø´ Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ØŒ ÙˆØ§Ù‚Ø¹Ø§ Ù‚Ø³Ù…ØªÛŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ø§Ø³ØªØŒ Ùˆ Ú©Ø¯Ø§Ù… Ø¨Ø®Ø´ Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ù¾Ø¯ÛŒÙ†Ú¯ Ù‡Ø³Øª\n",
        "\n",
        " Ø¯Ø± Ø§ÛŒÙ† Ù…ØªØ¯ Ø§Ø² Ù…ØªØ¯ Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ù‚Ø¨Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª\n",
        "\n",
        " Ø§Ø³Øª pack ØªÙ†Ù‡Ø§ Ù‚Ø³Ù…Øª Ø¬Ø¯ÛŒØ¯ Ø§ÛŒÙ† Ù…ØªØ¯ØŒ Ù…ØªØ¯\n",
        "\n",
        "\n",
        " ğŸŸª **pack** âŸ¶\n",
        "\n",
        "Ù…Ø´Ø®Øµ Ú©Ø±Ø¯Ù† Ø§ÙˆÙ„ Ùˆ Ø§Ø®Ø± Ø¬Ù…Ù„Ù‡\n",
        "\n",
        "    arr = [BOS_ID] + arr[:max_len-2] + [EOS_ID]\n",
        "\n",
        "Ù…Ø§Ø³Ú© ØªÙˆØ¬Ù‡ : ØªØ§Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡ Ø§Ø³ØªØŒ 1 Ø¨Ø²Ø§Ø±\n",
        "\n",
        "    attn = [1]*len(arr)\n",
        "\n",
        "Ø¯Ø± ØªÚ©Ù‡ Ú©Ø¯ Ø²ÛŒØ± Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø·ÙˆÙ„ Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ Ù…ÛŒÚ©Ù†ÛŒÙ….Ø§Ú¯Ø± Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡ Ú©Ù… Ø¨ÙˆØ¯ØŒ Ø¨Ù‡ Ø§Ù† Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒÚ©Ù†ÛŒÙ… (padding ):    \n",
        "\n",
        "    if len(arr) < max_len:\n",
        "      pad_len = max_len - len(arr)\n",
        "      arr += [PAD_ID]*pad_len\n",
        "      attn += [0]*pad_len\n",
        "\n",
        "Ù…Ø§Ø³Ú© ØªÙˆØ¬Ù‡ Ø¨Ø±Ø§ÛŒ Ù‚Ø³Ù…Øª Ù‡Ø§ÛŒ Ù¾Ø¯ Ø´Ø¯Ù‡ Ø¨Ø±Ø§Ø¨Ø± ØµÙØ± Ø§Ø³Øª:\n",
        "\n",
        "    attn += [0]*pad_len"
      ],
      "metadata": {
        "id": "dOtm0Cj7xhn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INPUT = 512\n",
        "MAX_TARGET = 128\n",
        "\n",
        "def apply_noise(text,do_sentperm=True,mask_ratio = 0.3,lam=3):\n",
        "\n",
        "  # Ù…ÛŒØªÙˆØ§Ù† ØªØ¹ÛŒÛŒÙ† Ú©Ø±Ø¯ Ú©Ù‡ Ø¬Ù…Ù„Ù‡ Ù‡Ø§ Ø¨Ù‡ Ù‡Ù… Ø¨Ø±ÛŒØ²Ù†Ø¯ ÛŒØ§ Ù†Ù‡\n",
        "  if do_sentperm:\n",
        "    text = sentence_permute(text)\n",
        "\n",
        "  # ids -> Ø§ÛŒÙ† Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¬Ù…Ù„Ù‡ Ø§ØµÙ„ÛŒ Ø§Ø³Øª.ÛŒØ¹Ù†ÛŒ Ø´Ú©Ù„ ØµØ­ÛŒØ­ Ùˆ Ø¯Ø±Ø³Øª Ø¬Ù…Ù„Ù‡\n",
        "  ids = tokenizer(text,truncation=True,max_length=MAX_INPUT,add_special_tokens=False)[\"input_ids\"]\n",
        "  if len(ids) == 0:\n",
        "    return None\n",
        "\n",
        "  noisy_ids = text_infilling_token_ids(ids,mask_ratio=mask_ratio,lam=lam)\n",
        "\n",
        "\n",
        "  def pack(arr,max_len):\n",
        "    arr = [BOS_ID] + arr[:max_len-2] + [EOS_ID]\n",
        "    attn = [1]*len(arr)\n",
        "    if len(arr) < max_len:\n",
        "      pad_len = max_len - len(arr)\n",
        "      arr += [PAD_ID]*pad_len\n",
        "      attn += [0]*pad_len\n",
        "    return arr, attn\n",
        "\n",
        "  noisy_imp, noisy_attn = pack(noisy_ids,MAX_INPUT)\n",
        "  clean_tgt, _  = pack(ids,MAX_TARGET)\n",
        "\n",
        "  return {\n",
        "      \"input_ids\":noisy_imp, # Ø´Ú©Ù„ Ù†ÙˆÛŒØ²ÛŒ Ø¬Ù…Ù„Ù‡\n",
        "      \"attention_mask\":noisy_attn, # Ù…Ø§Ø³Ú© ØªÙˆØ¬Ù‡ Ø´Ú©Ù„ Ù†ÙˆÛŒØ²ÛŒ Ø¬Ù…Ù„Ù‡\n",
        "      \"labels\":clean_tgt  # Ø´Ú©Ù„ ØµØ­ÛŒØ­ Ø¬Ù…Ù„Ù‡\n",
        "  }"
      ],
      "metadata": {
        "id": "c2rcYKj_M7Bm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nois_txt = apply_noise(text)\n",
        "print(tokenizer.decode(nois_txt[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhzEkmx64J4G",
        "outputId": "a7f9b4ae-20df-4cb6-91f4-696f58e6f61e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>Artificial Intelligence (<mask><mask> is a branch of computer science that<mask><mask><mask><mask><mask><mask><mask> tasks that normally<mask><mask> intelligence<mask><mask><mask><mask><mask><mask> reasoning<mask><mask>-solving, and understanding<mask><mask>.<mask><mask><mask> to develop, it raises both exciting opportunities and important ethical challenges for society. AI technologies are widely used today, from recommendation systems and chatbots to self-driving cars and medical diagnosis<mask></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **make_tensor_dataset**\n",
        "Ø§ÛŒÙ† Ù…ØªØ¯ Ø¯ÛŒØªØ§Ø³Øª Ø±Ø§ Ù…ÛŒÚ¯ÛŒØ±Ø¯ Ùˆ Ù†ÙˆÛŒØ² Ø±Ø§ Ø¨Ù‡ ØªÚ©Ø³Øª Ù‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒÚ©Ù†Ø¯.Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¨Ø± Ù…ÛŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Dataset ÙØ±Ù…"
      ],
      "metadata": {
        "id": "-7KC4ASA3j1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tensor_dataset(texts, n_limits=None):\n",
        "  rows = []\n",
        "  count = 0\n",
        "  for t in texts:\n",
        "    ex = apply_noise(t,do_sentperm=True,mask_ratio=0.3,lam=3)\n",
        "    if ex is not None:\n",
        "      rows.append(ex)\n",
        "      count += 1\n",
        "      if n_limits is not None and count >= n_limits:\n",
        "          break\n",
        "\n",
        "  input_ids = torch.tensor([r[\"input_ids\"] for r in rows], dtype=torch.long)\n",
        "  attention_mask = torch.tensor([r[\"attention_mask\"] for r in rows], dtype=torch.long)\n",
        "  labels = torch.tensor([r[\"labels\"] for r in rows], dtype=torch.long)\n",
        "  return {\"input_ids\":input_ids,\"attention_mask\":attention_mask, \"labels\":labels}"
      ],
      "metadata": {
        "id": "vhwWoNUvQtYI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = make_tensor_dataset(text)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMkaeEdQ4oEd",
        "outputId": "d01bb6c0-0058-4120-96c3-482e05ca5841"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         ...,\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1],\n",
              "         [    0, 50264,     2,  ...,     1,     1,     1]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'labels': tensor([[  0, 250,   2,  ...,   1,   1,   1],\n",
              "         [  0, 338,   2,  ...,   1,   1,   1],\n",
              "         [  0,  90,   2,  ...,   1,   1,   1],\n",
              "         ...,\n",
              "         [  0,  90,   2,  ...,   1,   1,   1],\n",
              "         [  0, 219,   2,  ...,   1,   1,   1],\n",
              "         [  0,   4,   2,  ...,   1,   1,   1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}