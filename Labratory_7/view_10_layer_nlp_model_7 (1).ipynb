{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3BsCS6Cvkqb",
    "outputId": "f43f7784-04be-4e64-c6f1-2615637c3ee1"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oRVQcqz__kZm"
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8M0WGWZz2dg_"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torchmetrics import Accuracy, F1Score, Precision, Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0xcR-q_78tf"
   },
   "source": [
    "## **1.Model_Components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd31gtYAoMFH"
   },
   "source": [
    "##### **SelfAttention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LuLkpNTH8JCQ"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, n_embd: int, seq_len: int, attn_pdrop: float= 0.0, resid_pdrop: float= 0.0):\n",
    "    super().__init__(self)\n",
    "    self.key = nn.Linear(n_embd, n_embd, bias= False)\n",
    "    self.query = nn.Linear(n_embd, n_embd, bias= False)\n",
    "    self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "    self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "    self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "    self.register_buffer('mask',torch.tril(torch.ones(seq_len,seq_len)).view(1,1,seq_len,seq_len))\n",
    "    self.n_embd = n_embd\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C = x.size()\n",
    "    k = self.key(x)\n",
    "    q = self.query(x)\n",
    "    v = self.value(x)\n",
    "    att = (q @ k.transpose(-2,-1)) / math.sqrt(C)\n",
    "    att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "    att = F.softmax(att,dim=-1)\n",
    "    att = self.attn_drop(att)\n",
    "    y = att @ v\n",
    "    y = self.resid_drop(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNFSzCYpoEaZ"
   },
   "source": [
    "##### **MultiHeadAttention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6vfC7xA_-8Ts"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self,n_embd:int,n_head:int,seq_len:int,attn_pdrop:float=0.0,resid_pdrop:float=0.0):\n",
    "    super().__init__()\n",
    "    assert n_embd % n_head == 0\n",
    "    self.n_head = n_head\n",
    "    self.head_dim = n_embd // n_head\n",
    "    # concatenated attention weights : (B, T, n_embd) -> (B, T, 3 * n_embd)\n",
    "    self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias = False)\n",
    "    self.out_proj = nn.Linear(n_embd, n_embd, bias = False)\n",
    "    self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "    self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "    self.register_buffer('mask',torch.tril(torch.ones(seq_len,seq_len)).view(1,1,seq_len,seq_len))\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    qkv = self.c_attn(x)   # Concat\n",
    "    q,k,v = qkv.split(C, dim = 2)  # dim = 2 -> ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒØ´ÙˆØ¯ C Ø¨Ø¹Ø¯ Ø³ÙˆÙ… Ø¨Ù‡ ØªÚ©Ù‡ Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡\n",
    "    q = q.view(B,T,self.n_head, C // self.n_head).transpose(1,2)\n",
    "    k = k.view(B,T,self.n_head, C // self.n_head).transpose(1,2)\n",
    "    v = v.view(B,T,self.n_head, C // self.n_head).transpose(1,2)\n",
    "    att = (q @ k.transpose(-2,-1)) / math.sqrt(C//self.n_head)\n",
    "    att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "    att = F.softmax(att, dim = -1)\n",
    "    att = self.attn_drop(att)\n",
    "    y = att @ v\n",
    "    y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "    y = self.resid_drop(self.out_proj(y))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hldUbbdUn9-X"
   },
   "source": [
    "##### **TransformerBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9_2n01Pacsxe"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, n_embd: int, n_head: int, seq_len: int, mlp_ratio = 4.0, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.attn = MultiHeadAttention(n_embd,n_head,seq_len,attn_pdrop,resid_pdrop)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "    self.mlp = nn.Linear(n_embd,n_embd)\n",
    "    \"\"\"nn.Sequential(\n",
    "        nn.Linear(n_embd,int(mlp_ratio * n_embd)),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(int(mlp_ratio * n_embd) ,n_embd),\n",
    "        nn.Dropout(resid_pdrop)\n",
    "    )\"\"\"\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x + self.attn(self.ln1(x))\n",
    "    x = x + self.mlp(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mjLMV8Onp2j"
   },
   "source": [
    "##### **ConvBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x36PkfCISBRv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, n_embd: int, pdrop= 0.0):\n",
    "    super().__init__()\n",
    "    self.ln = nn.LayerNorm(n_embd)\n",
    "    self.conv3 = nn.Conv1d(n_embd,n_embd,kernel_size=3,padding=1,groups=1)\n",
    "    self.conv5 = nn.Conv1d(n_embd,n_embd,kernel_size=5,padding=2,groups=1)\n",
    "    self.proj = nn.Linear(2*n_embd,n_embd)\n",
    "    self.drop = nn.Dropout(pdrop)\n",
    "    self.act = nn.GELU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    h = self.ln(x)\n",
    "    h = h.transpose(1,2)\n",
    "    y3 = self.conv3(h)\n",
    "    y5 = self.conv5(h)\n",
    "    y = torch.cat([y3,y5],dim=1).transpose(1,2)\n",
    "    y = self.proj(self.act(y))\n",
    "    y = self.drop(y)\n",
    "    return x + y\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M0DpnbAPHQYz"
   },
   "outputs": [],
   "source": [
    "class ConvBlock_3(nn.Module):\n",
    "  def __init__(self, n_embd: int, pdrop= 0.0):\n",
    "    super().__init__()\n",
    "    self.ln = nn.LayerNorm(n_embd)\n",
    "    self.conv3 = nn.Conv1d(n_embd,n_embd,kernel_size=3,padding=1,groups=1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    h = self.ln(x)\n",
    "    h = h.transpose(1,2)\n",
    "    y3 = self.conv3(h)\n",
    "    return x + y3.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CWoX1onfHvyl"
   },
   "outputs": [],
   "source": [
    "class ConvBlock_5(nn.Module):\n",
    "  def __init__(self, n_embd: int, pdrop= 0.0):\n",
    "    super().__init__()\n",
    "    self.ln = nn.LayerNorm(n_embd)\n",
    "    self.conv5 = nn.Conv1d(n_embd,n_embd,kernel_size=5,padding=2,groups=1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    h = self.ln(x)\n",
    "    h = h.transpose(1,2)\n",
    "    y5 = self.conv5(h)\n",
    "    return x + y5.transpose(1,2) # Transpose y5 back to (B, T, C) before adding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4eerMGGpS3r"
   },
   "source": [
    "##### **MLPBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQNMKU7LpRtP"
   },
   "outputs": [],
   "source": [
    "\"\"\"class MLPBlock(nn.Module):\n",
    "  def __init__(self,n_embd:int,mlp_ratio=4.0,pdrop=0.0):\n",
    "    super().__init__()\n",
    "    self.ln = nn.LayerNorm(n_embd)\n",
    "    self.fc1 = nn.Linear(n_embd,int(mlp_ratio * n_embd))\n",
    "    self.fc2 = nn.Linear(int(mlp_ratio * n_embd),n_embd)\n",
    "    self.drop = nn.Dropout(pdrop)\n",
    "    self.act = nn.GELU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    h = self.ln(x)\n",
    "    h = self.fc2(self.act(self.fc1(h)))\n",
    "    h = self.drop(h)\n",
    "    return x+h\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DoyTHurdIR86"
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "  def __init__(self,n_embd:int,pdrop=0.0):\n",
    "    super().__init__()\n",
    "    self.ln = nn.LayerNorm(n_embd)\n",
    "    self.drop = nn.Dropout(pdrop)\n",
    "    self.fc = nn.Linear(n_embd,n_embd)\n",
    "\n",
    "  def forward(self,x):\n",
    "    h = self.ln(x)\n",
    "    h = self.fc(h)\n",
    "    h = self.drop(h)\n",
    "    return x+h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VtMjQDvtMot"
   },
   "source": [
    "##### **MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Hpkyt_W0tTSA"
   },
   "outputs": [],
   "source": [
    "class HybridLM(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 10, \"This script assumes 10 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "        ConvBlock_3(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "        ConvBlock_5(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "        MLPBlock(n_embd,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "        MLPBlock(n_embd,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p07IQRNBujTx"
   },
   "source": [
    "## **2.Dataset-Wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ByZQB-hlu7w-"
   },
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "  def __init__(self,data_ids,seq_len):\n",
    "    self.data = data_ids\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) // self.seq_len\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    start = idx * self.seq_len\n",
    "    x = self.data[start : start + self.seq_len]\n",
    "    y = self.data[start + 1 : start + self.seq_len + 1]\n",
    "    return x, y # Corrected to return x and y separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kJLJR1dxMTu"
   },
   "source": [
    "## **3.Lightning-Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "6Wfse24-nS0g",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LitHybridLM_Models(pl.LightningModule):\n",
    "  def __init__(self,ModelClass,vocab_size,seq_len=256,num_classes=4,n_layer=10,n_head=6,n_embd=384,lr=3e-4):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.model = ModelClass(vocab_size, seq_len, n_layer, n_head, n_embd)\n",
    "    self.lr = lr\n",
    "    self.perplexity = Perplexity(ignore_index=-100)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    logits, loss = self.model(x,y)\n",
    "    self.log(\"train_loss\", loss, prog_bar=True)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    x,y = batch\n",
    "    logits,loss = self.model(x,y)\n",
    "    ppl = self.perplexity(logits,y)\n",
    "\n",
    "    self.log(\"val_loss\",loss,prog_bar=True)\n",
    "    self.log(\"val_perplexity\",ppl)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.AdamW(self.parameters(),lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET1vhvsVRnDu"
   },
   "source": [
    "## **4. Load Dataset & Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525,
     "referenced_widgets": [
      "cc7650c4d9af4d30a44c3016cc788285",
      "4e4281759d874616a5c9a649731a8c3a",
      "67deb1ae82f4483c8ff05e4c632753e0",
      "32a657445bd8466b817f01d7932cf823",
      "f4060ecc7347474e9889a8559a2344fe",
      "eb24003c688048b99bf891b17042629a",
      "486f554a23f249ebb782f4efa3c96d0a",
      "5b6b072588704765aaf2af3dd30161fa",
      "8e6d48ee493e4a52972b723708a6da9a",
      "520b4a5cc8e6428a9d3ac865d468352d",
      "8569f56bbb0248c9ab3082c9dd012ff0",
      "4686949164b540caa2921c84a0ee69fc",
      "28d4b2933cc2412dbebe80b99b44d555",
      "f383589a46ef439f979adcd3d7f4d85e",
      "1cad636064e541d99b75ed581d2e4872",
      "cc65330b166c4e10b6876d90acd24e0b",
      "1c4c9a9412eb4878b90c138cc5a9fa6f",
      "95caa8c7641f48039f3c616de3ac8d0f",
      "d2e834cf46964998b22cb7dfcf2e5714",
      "ac0f4e8fcf67484f8ad0534facd90245",
      "3886ca8b99c940aa845123698a05ff24",
      "2f767d444b3f486a9409f56a6884e83d",
      "8ed8c0d7b64944f490443f84f831f880",
      "467a03d1b7c740ef84bc5ca74d13434d",
      "f93e8becd3d64a56acd73752727bf9f9",
      "0e3d7c3d8d824658baa2e12b4124c39f",
      "3e999d8adb374f3fba7ad9a10da14184",
      "7d781e58eeea4c9ab70e742d255f44d6",
      "417bddbc839a487a9169a87a7a44c46a",
      "d93482e0060a41948e607db422a99f95",
      "8b6465e8082a43c1b7e84196c247f84c",
      "4879c0a67cbd4344b32f1ada9673fe86",
      "78079eed93b445fb8b1f52dfa8503c18",
      "864d710dd6224978a64c4bf677e704a5",
      "c639d4d6e292452b9475c089e03994a2",
      "b45cc682cd42426688150bad64ce14b1",
      "7d97937868f04cd5ad80a2f40dfae35b",
      "d59a628c34714a04b851c405dd887be2",
      "18109787aa0a463b976f92a8d81dadf8",
      "74568107d68d4ae691055a6c11ae9652",
      "da63fe6fed724de8b5e2eb45d5036978",
      "09a07c9cae90402ab9be6f2cbed085e5",
      "6b9711516f0a4babba4265e71ed22381",
      "3b5376d28f3042dea82793583df0eaf3",
      "d3648f4577a44c0db65d6fdfd27ba43c",
      "f059dbbf26f44e53a2cde700d5b5178a",
      "6b8288ca29f84cee8533725e737c71f4",
      "b58eac86a7324afdb5159a5460afdf62",
      "c4455a0e409b4e3a89a5abd9226c6419",
      "8a15645ac76b43b19cbf118086abf133",
      "e85e7363ce2b4fc6bfb572b72fe78d20",
      "3c83ad6ef56446febbab4fbd42adb95b",
      "e27493adb6904899afc0b3d7e7af6465",
      "635b59c6677a44cab3f14dd2f7a025cc",
      "e51032d4b7634d0e9870deeb03bd0b74",
      "c46b4351c50c428dba3b016bc2015f91",
      "20d2b23ba59c45b69a6c49c116e05982",
      "cd318490431946d78610d10de61eae66",
      "73b45a7f8af3420d8ce662a665cf9e39",
      "22f57e0a74634ddc95028cb6b04538e4",
      "24f59414912942b0bbe7828570f56405",
      "1e56aa16e38947ebba4c9a7fd1bdb5f0",
      "1799d84c0e134cd0bf9f3274bfdf5c0c",
      "72f8008f549b446a8e689495755c400a",
      "21e581d7dd714d1aa8a0cd9382c5b871",
      "06502d73e4404eaa9ec981fed0c740b7",
      "5ffeff7cfb8b4b8d848d0f32081a54bd",
      "fe27b281aa24470caf7011f4b57dbb45",
      "c31ce75ae1494281a78cc2f40944a104",
      "79413a060285494ea000839dc1df2fc5",
      "b511c4581a4e4c4a81e134ef901160e6",
      "d95b067d52b0432faa2013e3d0bf61ae",
      "6879e8df818345bcbfc72f362a0285d8",
      "781e64f984d3432fbb979e4952f36607",
      "5544756ef716485fa7dd308d3ca89b61",
      "d64578d85d7344a1b9a9f081dcdcf7ec",
      "e8c9c836791045b2aefb609ec6f1ee50",
      "b61818ee144142e5b06b1f68949f91fc",
      "dd20b9eb132a42d783aad0f4c6b272a4",
      "cc5b0d96948e417ab2457163d7969714",
      "6beb92d8c7614c70a1bb9e270c0523a5",
      "e0ce31ef81f74c0aba7018bbc86e1670",
      "55ceeb1ccb2e40fc81fba45f9e961eec",
      "022c4a956e16498b900eae2e7aa382f6",
      "058f0cec2bf747f8aaf907305593b1a8",
      "3d45fc7da26b4e4fa9b696bf32d32eeb",
      "0528abf88c6946e2897734dfbe2ab506",
      "3b36312726a841b9a95fa6aeac36a045",
      "86e539fd80c04155a00f37bee5c9a66b",
      "53b9094e8f2f4eda9b5c77336113baaf",
      "7cd5239c25d147f0a52a7afe187442dd",
      "6455d568d14f4a678c60c6621c26aec7",
      "1f12e811b3ad4260b08c611de68dfac6",
      "ea4fe9da4730402cadd8a309e36a8f8d",
      "3716c478070a463588bccfbc7a15c849",
      "758a3c0953464307bfd2480efe9b26ba",
      "e30f638443fa439e80c4877b42b86c7a",
      "a63583a78e144fb89d51bbd50aa53701",
      "8a2b4ddf111b4f8a8357f98944a175db",
      "b1323f16b6734bacaa8b712f531f1b0c",
      "424843b1d2eb40678cb9412478aeee73",
      "302418514f194b559d628918655172bb",
      "d1f781c321d54b00b857ffda9c894315",
      "2ad3e985bc624cdca9e33d5eaeadc06f",
      "fd3c3f0fa67f4a32b49ffdef45d44df9",
      "4cad462bf07c4c86bb8878073058a1c9",
      "d6ac4fb8a65547d5b73fca64f5a23954",
      "e3f7e224bcfa4d17947bf919f91e87c4",
      "b0094abf5b3d44038299d0b586e14fd2",
      "a67d03d50a434c5887b12060b143c45b",
      "0b02f78c318f4e25a7a9a9af138551fc",
      "9fed062223364da69278ae25c53ee1a3",
      "0a4d481e53644d0daa6100405d6d2469",
      "aa10a4ba76454fb2800034d2f352cea4",
      "3815b8259b984a1e9ede4408c61837d3",
      "60b502775eed4be0a12f88ac74927e75",
      "87d5198434734382a250350c3cfe1f0d",
      "a15b192d92a644749a19f1076f028a89",
      "4cfc7f7169e34391bfa402d84d9e6888",
      "0197f87df44148c6a62da7bd059450ff",
      "6274109e79c946bf8eedc5cb57abcf25",
      "518509803569465cac787ef0115dd8ef",
      "47c4d670b26d4ddabc22dc9febb9839f",
      "19e4f25b143e4cd5bb9fae4472e14289",
      "cd8ba6c20a7d4cd990da923d037ae175",
      "c597c1ec6c93492382729107b4717a33",
      "a1b5b97d05ca4d55bd17d8ed5008a8a2",
      "ad20dfbd53b24b42b657a8e296619456",
      "4fbacf1360284753af1d280d4ecca056",
      "82642d1a00ea4397b657100803cd57da",
      "5032c085244142c8a2d6cd30fd3941d2",
      "d1f60f02522d43d089886aaee4a375d1"
     ]
    },
    "collapsed": true,
    "id": "nazVhK6LQ5Ab",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "2445ded0-b57e-45fb-e8a6-91ff6b21594c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7650c4d9af4d30a44c3016cc788285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4686949164b540caa2921c84a0ee69fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/6.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed8c0d7b64944f490443f84f831f880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/641k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864d710dd6224978a64c4bf677e704a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/713k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3648f4577a44c0db65d6fdfd27ba43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46b4351c50c428dba3b016bc2015f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffeff7cfb8b4b8d848d0f32081a54bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61818ee144142e5b06b1f68949f91fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e539fd80c04155a00f37bee5c9a66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1323f16b6734bacaa8b712f531f1b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b02f78c318f4e25a7a9a9af138551fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518509803569465cac787ef0115dd8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"mikasenghaas/wikitext-2\")\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdNHVoB5yxRl"
   },
   "source": [
    "## **5.Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ShUyddmBzHnP"
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "  enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "  return enc[\"input_ids\"].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JcotxR6hzKxg"
   },
   "outputs": [],
   "source": [
    "def data_loader(train_size:int,val_size:int,seq_len:int = 256 , batch_size:int=24):\n",
    "  train_texts = dataset[\"train\"][\"text\"][:train_size]\n",
    "  val_texts = dataset[\"test\"][\"text\"][:val_size]\n",
    "\n",
    "  train_ids = encode_texts(train_texts)\n",
    "  val_ids = encode_texts(val_texts)\n",
    "\n",
    "  #seq_len = 256\n",
    "  #batch_size = 24\n",
    "  train_ds = TokenDataset(train_ids, seq_len = seq_len)\n",
    "  val_ds = TokenDataset(val_ids, seq_len = seq_len)\n",
    "\n",
    "  train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "  val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "  return train_loader,val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0Dm2Fo020lih"
   },
   "outputs": [],
   "source": [
    "def train_method(ModelClass,train_size:int,val_size:int,n_layer:int,seq_len:int=256,batch_size:int = 24):\n",
    "  train_loader,val_loader = data_loader(train_size,val_size,seq_len,batch_size)\n",
    "\n",
    "  lit_model = LitHybridLM_Models(ModelClass= ModelClass,vocab_size=len(tokenizer),n_layer=n_layer)\n",
    "\n",
    "  trainer = pl.Trainer(\n",
    "      max_epochs=2,    #  epoch = 3\n",
    "      accelerator=\"auto\",\n",
    "      devices=1,\n",
    "      precision=16,\n",
    "      log_every_n_steps=10\n",
    "  )\n",
    "\n",
    "  trainer.fit(lit_model, train_loader, val_loader)\n",
    "  return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgAUP0LzBCBq"
   },
   "source": [
    "## **6.Train**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "d52cb500770743599ace52beaf968ba1",
      "33de51516bb64b9c8b002cadd208e74c",
      "1db761c7ff2840ec831c9aaea7077103",
      "b6b26b24633746b3a6336222956a8ac5",
      "9f51d28268bc40389ce4b7cb9206a81f",
      "4412e9bee8e64cc29c9c80caf2d1a223",
      "cd1fb6a1c68d47e09bd914c955762305",
      "f1c16cba13ee49c28c9189ba819bb529",
      "49aed1f5a0e84f08a6bd3ff03b2ad0fc",
      "777bbc6a6ecf4207bc8b159810e63b30",
      "3abfc3ec39124ca5a793cf62f858d311",
      "69105ba4278f45fa8cb6569654f774bb",
      "37fe02f82a644f6981b53efbd5e4776c",
      "e16cb4d54b194f15b565d8e4351e6459",
      "5f262168f9144d619c43f5f516909d2f",
      "f5ca4add0dfd40b9ac8d87b3a9be22db",
      "2584a94fe4d543b4ab55442753b7bc20",
      "e67be4dc9766458481aba59e439b9b46",
      "dc2135c777234e1eaa5f4f8a8915ce3d",
      "051d7c519fcb4fff98450c7abc74afe6",
      "b9ec36c8158449538387a5c8534f739f",
      "4c570683f97a473ea851d60f65217732",
      "a9e61bf95ae14ed7a8809a7ca9536e21",
      "1d59d17f0781423bac0be771d91cfc9e",
      "1f38c48818df43bea737b92afbf90dee",
      "b23994944a6e473fbb7018dfecbcd1c6",
      "6086db354082421189aaf09d0246d918",
      "12a097971eaa44c79fd706a5d3bbd251",
      "5dda27d41c1d4c9c85f882d2b8d58bdd",
      "e95228d84a0244ecb29e5ae111bf5e29",
      "32139cc1869343bba11992df8b1255a3",
      "2e9387da8fbc4f3eb3b533cbacfdf94a",
      "148bfd4e3ff84abb8eb8798e0fa02217",
      "f8bb09cb349744f6a650549d8ce9c1af",
      "31f61130b43f4a6a89836caf029029b4",
      "bce9a92858d94aab919f736a3a8754d6",
      "ff47463f9a3f43ae985ecb1c7a66a1b2",
      "8cb8f1d0e26c456bbff3b0427d42d3fc",
      "61c445f2db414e519771347054568d59",
      "d53b043c830143f398093e64736661c5",
      "4d4c71fb768f44f7a64edc684114b568",
      "d184e897089b4e41a355ac051bec155f",
      "1604a9c82c644a0b8ee92acb47c4a11b",
      "6b5b961c7e564fa0a6951fc21bf6958e"
     ]
    },
    "id": "4S8-zcvzsQKD",
    "outputId": "5e1c5586-a4d2-4101-a48a-746b372d3959"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `Perplexity` from `torchmetrics` was deprecated and will be removed in 2.0. Import `Perplexity` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/usr/local/lib/python3.12/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM    | 44.6 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "44.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.6 M    Total params\n",
      "178.443   Total estimated model params size (MB)\n",
      "75        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52cb500770743599ace52beaf968ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69105ba4278f45fa8cb6569654f774bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e61bf95ae14ed7a8809a7ca9536e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bb09cb349744f6a650549d8ce9c1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = train_method(ModelClass= HybridLM,train_size=30000,val_size=5000,n_layer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eefnctV5YlsZ",
    "outputId": "0c7a5e93-832b-4ac3-b488-cec206fd574d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0261 | Perplexity : 1.03\n"
     ]
    }
   ],
   "source": [
    "val_perp = trainer.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss:.4f} | Perplexity : {val_perp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOY16qFdSGFA"
   },
   "source": [
    "## **7.Labratory(4)**\n",
    "\n",
    "Dataset : __\"mikasenghaas/wikitext-2\"__\n",
    "\n",
    "Changing the model architecture,\n",
    "Changing train_size and val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xE-cjwuTxY3"
   },
   "source": [
    "### **Train(2)**\n",
    "\n",
    "model ( 1 Layers ) :    MLPBlock\n",
    "\n",
    "train_size : 30000  \n",
    "\n",
    "val_size : 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqw02XfeTxY4"
   },
   "outputs": [],
   "source": [
    "class HybridLM_2(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 1, \"This script assumes 2 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "        MLPBlock(n_embd,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "     #   MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "     #   TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "2d695935158a456691efbdb71a44144e",
      "fc649c56d9d742cfa10b4e9c5ef68ff4",
      "ce37b2778eff4181bf0ad7b4950a714b",
      "3b59654d86fc46319c024baae4099cff",
      "6629d5c3461146b081e6fe647058b38c",
      "dc42e2fe2a6546cabd93199e79e73047",
      "e08d84ed94f5400aa2d65d03f476fe47",
      "1067dc3af038446fa25f84a77d09033b",
      "0122c803c2f84b5eaf53c4e26a083ec4",
      "0ef6563ca3d54ff9bb4496990fd65d9a",
      "ff75eb83a71a4bc4ba4aed6495c0b2d0",
      "aafd39b947b94895b24721faa6116be0",
      "b385f86907ff4ebf97e29b9095fbf003",
      "ad98152266514a7e88a93b87986d7ee4",
      "cd710aaaab674745baa336e24e80706f",
      "e05b3f3e1e2c4ab58e1e1e0bde8e092c",
      "19afd29c4c6d4e3b89d47a4768443352",
      "be1644b8eefe4bf89c5c0398bd8d7d1d",
      "f1491636789f4f97b33c7f2895e18032",
      "c2e03a71aacf4ae2a98a53198e2376f7",
      "655468b185b44fa8bd21a9b7e8e0e906",
      "68ebeac0fa4741a391492d7d6b2e3ddc",
      "240ce955adb648bdb5bca66ad667c283",
      "a04a954909fd4d32a7790132b4f678e0",
      "c19214f9b7a64623a53d98c020d3c59f",
      "5cdc9dea6f1b4958bb467559b644d3af",
      "2ada19c706e146dca537c5d907b655fa",
      "4b3e922052284a57af3a611f19182b72",
      "a7961a38979e44cabb44caaf6b145e02",
      "82f16d5d318941bbaff953347fe190c9",
      "3174f488e6d142f6b4e7840c800c41c5",
      "c9009cda09774d98b9b99b0db96b74e0",
      "bd93148e99fa4c47ae16b2b7970b168b",
      "42bf2ab6a4854464a9a8828207fae91a",
      "215c0e0862444c099aa48b3307a5e10f",
      "9f631bc572724108b83e4199f8c2a61d",
      "6b5ba8458e6448f3bfdd1c9cf2935a3b",
      "141435d1c53e4dcaa58732244cddbc9f",
      "523570ed076c42aea352ae61d2ad1ff6",
      "8631713e8aca4b9a854664b5fec69319",
      "a58a0d6abbbf44708281083e4c6d3c3b",
      "eb2513bb870b404bbcfc6facb0661c32",
      "5813b2d46776491594ec015693638b51",
      "3ff5864af4b4416da1bb7ef8f449f51b"
     ]
    },
    "collapsed": true,
    "id": "mIyxudTPTxY5",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a747e352-aad5-417a-a2f2-c5ef13025852"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `Perplexity` from `torchmetrics` was deprecated and will be removed in 2.0. Import `Perplexity` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/usr/local/lib/python3.12/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM_2  | 38.8 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "38.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "38.8 M    Total params\n",
      "155.380   Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d695935158a456691efbdb71a44144e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafd39b947b94895b24721faa6116be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240ce955adb648bdb5bca66ad667c283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bf2ab6a4854464a9a8828207fae91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_2 = train_method(ModelClass=HybridLM_2,train_size=30000,val_size=5000,n_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eF6jO9k7TxY5",
    "outputId": "2b95fb37-4cbe-405f-d4d0-942a20d97185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 1.4041 | Perplexity : 5.11\n"
     ]
    }
   ],
   "source": [
    "val_perp_2 = trainer_2.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss_2 = trainer_2.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss_2:.4f} | Perplexity : {val_perp_2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cw_L6n5GhwXk"
   },
   "source": [
    "### **Train(3)**\n",
    "\n",
    "model ( 5 Layers ) : ConvBlock_3 -> TransformerBlock ( * 3 ) -> MLPBlock\n",
    "\n",
    "train_size : 30000\n",
    "\n",
    "val_size : 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04wx33nxhwXl"
   },
   "outputs": [],
   "source": [
    "class HybridLM_3(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 5, \"This script assumes 5 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "        ConvBlock_3(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "     #   MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "        MLPBlock(n_embd,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "     #   TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "55afd57fdabe4729a9fa5599a58c26b7",
      "6e45ae29e7bc4b3ebde8afecd7cb72b4",
      "9899f59f9b694ae8a5a3fcf5af2710e3",
      "1bf94eb83ea241c7a1bd0fcc89ea806b",
      "ddf53f12f3ba4177a3f561168e96fcbd",
      "71896630c11e476b8e2872974cffaf39",
      "66f6573f39654b68b3626ae87e8a206c",
      "ebc85b3653224dd299e6f5a4068ac3bf",
      "70469a2a00b74c36807a14dbbe1b9c9e",
      "f470bd95ff1c408896c877f5525c4b91",
      "faa3c111f4524b9cb792d6edd427adec",
      "969b4b417c0f4a73aabad8f3fe22cdec",
      "45ef50f409244bb8a00365b179bbd1c5",
      "f217182068c844a29e17ee128612df9f",
      "db55a86aeb8c4f63886bc68f6c4be7c8",
      "4a3734abf3a54f42baeb33aefbeeb958",
      "90dd640bf90148fb8eb944ff90fcacc9",
      "c3b1c84aba2e4787b18067ea34497e77",
      "22d2dd45b65b4c6c9d0a4fb9e7f24705",
      "0f8267d7da2b431ebafdb9edd537043e",
      "e59a1f1017cb43babe0ba1b3015a5e60",
      "845003ddd73a429a9f5566f8ef12ad91",
      "1cc400e817e04e1890fd15fe372e9c4a",
      "be39ca14eca74493b120a72fec77bf41",
      "c8948e35f4e14852b0af750f300257ab",
      "e3399d0ad12449fda74bdd3255ea1a9a",
      "45ddf04c70544df88f8e4cb53cbcfc39",
      "6370ac0524d949b68794ff542b097028",
      "e928c603d3564ceab1541068261eee8b",
      "b6650b7b6c1444cdb1d6ddfd4670fe24",
      "aeffafea75284d21a959321a88d6660d",
      "5c94a54ba833455a96bdd73671c30c69",
      "12da725e113d4ad18e47350bcba46e26",
      "41705381445e4b169c7e296738d0976e",
      "26f35bf10dbf46a5a9b42c4ae60053fe",
      "8ca6ca3079d8438f9286f09f9ea8290d",
      "fef31c9eb63142f787082b420e8543c8",
      "ec4cfee7bd7a40ecbde14d3d51b87df4",
      "06b480e0b0204d49a2b04e2162759f59",
      "a57617dd22e1446895959c7e1dee8ae2",
      "a037624bc7114067838c2366f6fdc182",
      "e540a505be9144fb8df7c10d8f20f0df",
      "a903b9a09902454994e80ab597197ec8",
      "09d485efa12f4b9c9045df66b783ed70"
     ]
    },
    "id": "GHeoFZrOsAPc",
    "outputId": "8156b7cc-b229-4ef1-c34d-674bcfa149cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM_3  | 41.5 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "41.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.5 M    Total params\n",
      "166.025   Total estimated model params size (MB)\n",
      "41        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55afd57fdabe4729a9fa5599a58c26b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969b4b417c0f4a73aabad8f3fe22cdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc400e817e04e1890fd15fe372e9c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41705381445e4b169c7e296738d0976e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_3 = train_method(ModelClass=HybridLM_3,train_size=30000,val_size=5000,n_layer=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kKZ9Vv9hwXl",
    "outputId": "9d512624-2a47-483f-bc88-0bd85c4bc940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0254 | Perplexity : 1.03\n"
     ]
    }
   ],
   "source": [
    "val_perp_3 = trainer_3.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss_3 = trainer_3.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss_3:.4f} | Perplexity : {val_perp_3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMu9LNdPop2L"
   },
   "source": [
    "### **Train(4)**\n",
    "\n",
    "model ( 1 Layers ) : ConvBlock_5\n",
    "\n",
    "train_size : 15000\n",
    "\n",
    "val_size : 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qx12TU6bop2M"
   },
   "outputs": [],
   "source": [
    "class HybridLM_4(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 1, \"This script assumes 5 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "        ConvBlock_5(n_embd,pdrop=resid_pdrop) # 2, changed dropout to pdrop\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "     #   MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "      #  MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "     #   TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "453ed595d18947c8a1b7b138f5660d08",
      "d2f5f316916848ff8d1649fabb2667b1",
      "300a2b2bb66042798e92ac7e2843ba39",
      "c71f3e2d9f2e4bd98d13d87222ac209f",
      "b9f3b5854a4b4494b628978d06a756de",
      "fb9574176c0f427c9ec1c8b654c4bdd9",
      "f4a4f84fe96347b086eb797a6e6a7129",
      "a8f07ee53ad34a2d97f883b4efd0f430",
      "129ab311615141059bba1babab1f5417",
      "b6ca494eea2d439eaa9985061a617ebe",
      "b40e8cc07d85401c8ddb13b8517475de",
      "89f14a44abfd489ba5aca56d7ee14cf0",
      "9d23d479efd14d8ba906d144ff25fdff",
      "7bd2a9d706d34abb9b1b92268a100a5c",
      "9ad76c4e680a4dd3a434351d2a60a57f",
      "da69096eee714ceba0a7ce8530112ff8",
      "a347a4a8cd3b4d9c8b275b451bad7257",
      "4743f4bf9e2a4c8eb0903ac97e084af3",
      "144bed3153e948cead28d4316525cb1a",
      "a26f39eb5ae44be9ad9ded7b39feac3f",
      "02148738a2694c20acd8b49efb6c6d34",
      "81554c9b9be248d091ac914cf6b88296",
      "a4746e4b01ed4e768c6bc4093038c292",
      "259945d33e72416c8daff5a076be6764",
      "57ba28042dc74bdda79ca4a351da7500",
      "0623892f254040dbb3577e4315993d36",
      "254575284e394a329338db4d6f9fa1a9",
      "815c69fcd8254e63af5d725d67342d55",
      "5e8c0392f8af40418aac6f7e22acec55",
      "70a583e96d34453183c4b2215a5bbceb",
      "e7dfebe6601b4e8d867d84f024dd1ad2",
      "4ffd09ba2ea64982ba64718b84ce7fa8",
      "370ce7f28aa24da482cd5ff382e37d4b",
      "01ef176f12634f85b55869524be1c144",
      "f60ab0ffcba347129e7d52e7dd56d377",
      "4f35634b71eb451cab944ca15a0c1264",
      "966dfd625e6e490b8d2cde13fa5e2941",
      "30ab94683cf342dbae2e870d7c04c686",
      "09156419092a4e6cb67f142c3079ff53",
      "a2c68d84736748f0a236a8474ca954b9",
      "2caf3c5a682848fe9d4c8b65dfa72c7f",
      "68e4dc75308147bda57fa8e777beec5b",
      "6655227c07ba4183b64a83baf4b5ce12",
      "beee45f1eb664404ab01d6e399d415c6"
     ]
    },
    "id": "z5jIovAKop2M",
    "outputId": "4f18a12d-064b-4d24-85a6-660f1d3ebcd3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `Perplexity` from `torchmetrics` was deprecated and will be removed in 2.0. Import `Perplexity` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/usr/local/lib/python3.12/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM_4  | 39.4 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "39.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "39.4 M    Total params\n",
      "157.740   Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453ed595d18947c8a1b7b138f5660d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f14a44abfd489ba5aca56d7ee14cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4746e4b01ed4e768c6bc4093038c292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ef176f12634f85b55869524be1c144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_4 = train_method(ModelClass=HybridLM_4,train_size=15000,val_size=2000,n_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2A6LcOuqop2N",
    "outputId": "47a03953-1ddc-4b90-c8ac-50d7dbfe6bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0299 | Perplexity : 1.03\n"
     ]
    }
   ],
   "source": [
    "val_perp_4 = trainer_4.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss_4 = trainer_4.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss_4:.4f} | Perplexity : {val_perp_4:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90vmSlLPs9oB"
   },
   "source": [
    "### **Train(5)**\n",
    "\n",
    "model ( 1 Layers ) : TransformerBlock\n",
    "\n",
    "train_size : 30000\n",
    "\n",
    "val_size : 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LURgIJR4s9oC"
   },
   "outputs": [],
   "source": [
    "class HybridLM_5(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 1, \"This script assumes 5 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "     #   MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "      #  MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "     #   TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "6860620bed1f4d8e9a3fa5e1cef764d6",
      "47c783e875494f7b986697607373307d",
      "48994711c9ac45a39f1afeabbfef837b",
      "e286eadd309d479b81ea9bf29cff1510",
      "dc686f7b4a7445ce8f415153dee22976",
      "dfe0b386387c45848919d60a77d69043",
      "16d842132a5b4aecb810e33de850b54c",
      "348a5eb4df1d4fa5b76d66e1f67220a7",
      "c56af1a804f34a69aa23fd149a2d198a",
      "43a8acc55adb46f8a8ed4e0d1372b4d6",
      "3a8a29f597a146be9a5b5b3263cfeb2b",
      "c2c327b306a0478780ad76696a8c1712",
      "0dd1488b743f441ea3097e6d3681f977",
      "a606e4b1f39e499c9d05fdf82f9d3490",
      "23f29a4194494f5b8f610a64ada98d19",
      "f4df0c71c3d844fe9e5373103256374a",
      "3a24a7a8df4d48b98bc8bc12fb94588e",
      "2f2eb2071ffa4912b9ed9fe7966017a6",
      "d10bd226ec87415192d1f6a54427faa8",
      "a9c59d34d1744e2e827d53455020f26f",
      "0909652dfae5499f8d78dd7d4247b64b",
      "454a94610f6a456aa8b643e109ab5c84",
      "ddb0238c854d4a19981d9fb1aa937a99",
      "59b91bc3e4014b1ca93cb4aa0dde3c98",
      "2e6b7f0e3741462a80add3ddbe82ef1a",
      "ea3c4a165eff477290032b56c62a288b",
      "959674f4c98f439188ed02d09605e468",
      "d84f0cbe6c7941a4abab65f3678ad00c",
      "a70ef39e770040e080ea6207b8497098",
      "c774f79985524cdeae73ceda918f4626",
      "01ae7b613d564f0e86a91b017bc9d0b2",
      "c4bf8cb385a240a080788f80cd58c077",
      "c49b61850d2240aea3505a1dbefe0712",
      "51275a9a543c4e40b6d48c259588cbaf",
      "b92e70b7099040ddbb01663e4929c043",
      "4c3cedef27de4b569fac6053eefeedca",
      "931d01184dd347eeb8fa21b7a7df1f81",
      "e36519975d9f4f4fa018779617d94e61",
      "0ad872bf0e084d579fef9d40a5adcd89",
      "f6b66f6a9c06460bb500617af802e964",
      "cd0fe6517e82430589997facd426b52c",
      "48d03540d0c8444381baa3a199ba5c65",
      "61801c02ce4c4b73bd9bb5940436345d",
      "a66545ce191448378e82d9eda667cbda"
     ]
    },
    "id": "dzDC4Cwds9oD",
    "outputId": "c6164690-93c2-4e5e-9691-cecdda0605e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM_5  | 39.4 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "39.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "39.4 M    Total params\n",
      "157.743   Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6860620bed1f4d8e9a3fa5e1cef764d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c327b306a0478780ad76696a8c1712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb0238c854d4a19981d9fb1aa937a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51275a9a543c4e40b6d48c259588cbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_5 = train_method(ModelClass=HybridLM_5,train_size=30000,val_size=5000,n_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiOvOJocs9oD",
    "outputId": "fde6dc12-dd39-4b08-f569-3458bb7a27fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 1.3355 | Perplexity : 4.67\n"
     ]
    }
   ],
   "source": [
    "val_perp_5 = trainer_5.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss_5 = trainer_5.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss_5:.4f} | Perplexity : {val_perp_5:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "cells": [],
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.10.12"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
