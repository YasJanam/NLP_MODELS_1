## Labratory_7
بررسی عملکرد مدل کد 3 با تغییر مدل های بلاک های استفاده شده در معماری مدل اصلی ( MLPBlock , TransformerBlock , ConvBlock ) و کاهش ایپاک آموزش از 3 به 2

__در کل ساده سازی صورت گرفته__

تغییرات ایجاد شده در خود کد نیز قابل مشاهده و مقایسه است

بعد ساده سازی های صورت گرفته دوباره عملکرد مدل در train های مختلف با لایه های متفاوت بررسی میشود

#### ConvBlock 
کلا این بلاک حذف شد و به جایش از دو بلاک ConvBlock_3 , ConvBlock_5 استفاده میشود .

معماری ConvBlock  یک شبکه عصبی ساده بود که از دو کانولوشن با سایزکرنل 3 و 5 و یک لایه خطی استفاده میکرد.

اما ConvBlock_3 همان کانولوشن با سایز کرنل 3 است و ConvBlock_5 نیز همان کانولوشن با سایز کرنل 5 است.

#### MLPBlock 

 بلاک MLPBlock نیز ساده تر شد ( دربلاک جدید تنها یک لایه خطی استفاده شده )

#### TransformerBlock

قسمت MLP ترنسفورمر تغییر کرد و با یک لایه خطی جایگزین شد.

#### epoch_num : کاهش از 3 به 2



---

### 📊 Results 
<img width="1695" height="362" alt="results" src="https://github.com/user-attachments/assets/38772b87-daad-4a7b-b4be-773444de4c4d" />


---

در کد های قبلی با بلاک های قبلی مشاهده میکردیم که نتایج یک بلاک ترنسفورمر با نتایج مدل 10 لایه مشابه هم میشدند اما اکنون میبینیم که دیگر اینطور نیست. یا مثلا __در کد های قبلی MLPBlock برای یادگیری دیتاست کافی بود__ و عملکردی مشایه مدل 10 لایه داشت اما اکنون به وضوح اینطور نیست
