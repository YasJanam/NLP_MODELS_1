{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhPD8zU1s6WD"
   },
   "source": [
    "### **Evaluation**\n",
    "Ø¯Ø§Ø±ÛŒÙ… generate Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…ØªØ¯\n",
    "\n",
    "Ø§ÛŒÙ† Ù…ØªØ¯ Ø±Ø§ ØªØ¹Ø±ÛŒÙ Ù†Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯ÛŒÙ… MiniBART Ø¯Ø± Ú©Ù„Ø§Ø³\n",
    "\n",
    "Ø§Ø±Ø« Ø¨Ø±ÛŒ Ù…ÛŒÚ©Ù†Ø¯ MiniBART Ø§Ù…Ø§ Ø§Ú©Ù†ÙˆÙ† Ù…Ø¯Ù„ÛŒ Ø¬Ø¯ÛŒØ¯ ØªØ¹Ø±ÛŒÙ Ù…ÛŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø§Ø² Ú©Ù„Ø§Ø³\n",
    "\n",
    "Ø±Ø§ Ø¯Ø§Ø±Ø¯ generate Ø§Ø³Øª Ùˆ Ù…ØªØ¯ MiniBARTwithGen Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø¬Ø¯ÛŒØ¯ Ù†Ø§Ù…Ø´\n",
    "\n",
    "Ù…Ø¯Ù„ Ø­Ø§ØµÙ„ Ø§Ø² Ù„Ø§Ø¨Ø±Ø§ØªÙˆØ±ÛŒ 13 Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù„ÙˆØ¯ Ù…ÛŒÚ©Ù†ÛŒÙ… ÙˆÙ„ÛŒ Ú©Ù„Ø§Ø³ Ø§ÛŒÙ… Ù…Ø¯Ù„ Ø±Ø§ Ú©Ù„Ø§Ø³ Ø¬Ø¯ÛŒØ¯ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ù…ÛŒÚ¯ÛŒØ±ÛŒÙ…\n",
    "\n",
    "ðŸŒ‘ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ø¯Ùˆ Ø¯ÛŒØªØ§Ø³Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ØŒ Ø¯Ø±Ø­Ù‚ÛŒÙ‚Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ùˆ Ù…Ù†Ø¸ÙˆØ± Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ´ÙˆØ¯:\n",
    "1. wikitext -> Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ú†Ù‚Ø¯Ø± Ø²Ø¨Ø§Ù† Ø±Ø§ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡\n",
    "2. cnn/daily-mail -> Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ú†Ù‚Ø¯Ø± ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø®Ù„Ø§ØµÙ‡ Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø±Ø¯\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZS75kBVtysOZ"
   },
   "source": [
    "ðŸ”½  **eval-results** ðŸ”½\n",
    "\n",
    "| dataset | test_size | rouge1 | rouge2 | rougeL | rougeLsum |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| wikitext | 3000 | 0.1121 | 0.0142 | 0.0890 | 0.0769 |\n",
    "| cnn/dailymail | 2000 | 0.0934 | 0.0087 | 0.0775 | 0.0893 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HE_1cwdkw87d",
    "outputId": "3822e2f2-49b4-4ccd-dd2b-9b24985bc77c"
   },
   "outputs": [],
   "source": [
    "!pip install lightning evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "skRYnA_zkIwg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import evaluate\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "f6c221fd2b2d4c7684c5cc50c4e97e2b",
      "fffd13947db3440ea0e650d327e70eda",
      "b11e071cae6e4517a4b83529b42ddef2",
      "b8e75499ab2e4a8c96377e3da5470e05",
      "58e785bf2b864dc09fed774fb52ef202",
      "d2ece169b8a74a4295ed25162d1b7096",
      "2ea3f2086a844ad498860edb890a8a80",
      "4bf62c28dcda4ce699909acba7cbf81b",
      "fe8261f3262f46d4be11722f4872f53b",
      "050a793c9eff4bcd8f8e389a19e55a03",
      "a012eb023ec2423e8eb4fe19e40a4d92",
      "983b836a6a074fcfbf88243b2838599e",
      "ca742ce1e5e945a2be27ba27055c3d7a",
      "a6071cdec2a542ff8f2a69d8fdc777b8",
      "d35d575b47ab4cf2885641ca8a838440",
      "ab5d4b7c827649b79efd01690bd398ae",
      "528cd99e9c1c4777869c41794ebac80c",
      "cf7f544f2024453381f71859ebf776ef",
      "c62f93e858dd4ef18a0cc07b8ff51183",
      "20d428cde497406d9534b1bcfd38a8d3",
      "4e35d529d0ef4fc58291ec7ef25a547e",
      "8766a1356f694a2e938af8bd04260c4b",
      "4448bb372f78417882dff9b3f1576558",
      "cf84a3f9382643c088a8013d99832fe7",
      "980a0f99a60144eeafa09e0ca53be2b7",
      "6e96c77034b244c58112ab8ebc533d03",
      "1ef8a8befe4449ee94d76d209928a050",
      "0f6dc7b4a19f4defb1c85267620f6c46",
      "3b1f1c17df7441b09a39a1765c82af08",
      "c0e1b80c6e86465ca918b46c61b6e6f4",
      "5c7dbd85df9e4dd4b0171dc52d3eba5a",
      "bd64d6b38cd94bea98ed45d8d9d44a74",
      "059d43c4d5754dd68d8b1fff0383b6f4",
      "eeaf947e02f742b8be4d532a878b20d2",
      "491875dfddc84e47a65b5c833574bbda",
      "5506f3f378594c2394325e7ba7e73e11",
      "4bf513097d5e4373972fd8abed2a33f2",
      "05ef11844afc4a879642a52bdeef29f9",
      "53fb805f353e41efbf636c7c1e344ff2",
      "f4e845a215384100a240fb0f14117ad9",
      "2322b487d2d342f48c008336bd9ffcb4",
      "9d77c324a64342cc912f426ee46a1f08",
      "aae785e750d64ac690ff224d8248d2d5",
      "c97ed73b980c402b888cdc0e691592cd"
     ]
    },
    "id": "idFvXQRtb8Px",
    "outputId": "348de22b-8337-42ab-c672-7d25f46ba2db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c221fd2b2d4c7684c5cc50c4e97e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983b836a6a074fcfbf88243b2838599e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4448bb372f78417882dff9b3f1576558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaf947e02f742b8be4d532a878b20d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "MASK_ID = tokenizer.mask_token_id\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "EOS_ID = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_paps_dtVSk"
   },
   "source": [
    "### **BART-Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L9IP8tzHkV_-"
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.lin1 = nn.Linear(d_model,d_ff)\n",
    "    self.lin2 = nn.Linear(d_ff,d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  def forward(self,x):\n",
    "    return self.lin2(self.dropout(F.gelu(self.lin1(x))))\n",
    "\n",
    "class SelfAttn(nn.Module):\n",
    "  def __init__(self,d_model,n_heads,dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.attn = nn.MultiheadAttention(d_model,n_heads,dropout=dropout,batch_first=True)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "  def forward(self,x,attn_mask=None,key_padding_mask=None):\n",
    "    h,_ = self.attn(x,x,x,attn_mask=attn_mask,key_padding_mask=key_padding_mask,need_weights=False)\n",
    "    return self.ln(x+h)\n",
    "\n",
    "class CrossAttn(nn.Module):\n",
    "  def __init__(self,d_model,n_heads,dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.attn = nn.MultiheadAttention(d_model,n_heads,dropout=dropout,batch_first=True)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "  def forward(self,x,mem,attn_mask=None,key_padding_mask=None,mem_padding_mask=None):\n",
    "    h,_ = self.attn(x,mem,mem,attn_mask=attn_mask,key_padding_mask=key_padding_mask)\n",
    "    return self.ln(x+h)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self,d_model,n_heads,d_ff,dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.self_attn = SelfAttn(d_model,n_heads,dropout)\n",
    "    self.ffn = FFN(d_model,d_ff,dropout)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "  def forward(self,x,key_padding_mask=None):\n",
    "    x = self.self_attn(x,key_padding_mask=key_padding_mask)\n",
    "    return self.ln(x+self.ffn(x))\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self,d_model,n_heads,d_ff,dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.self_attn = SelfAttn(d_model,n_heads,dropout)\n",
    "    self.cross_attn = CrossAttn(d_model,n_heads,dropout)\n",
    "    self.ffn = FFN(d_model,d_ff,dropout)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "  def forward(self,x,mem,tgt_key_padding_mask=None,mem_key_padding_mask=None,causal_mask=None):\n",
    "    x = self.self_attn(x,attn_mask=causal_mask,key_padding_mask=tgt_key_padding_mask)\n",
    "    x = self.cross_attn(x,mem,mem_padding_mask=mem_key_padding_mask)\n",
    "    return self.ln(x+self.ffn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uSoM-C6oqJXL"
   },
   "outputs": [],
   "source": [
    "class MiniBART(nn.Module):\n",
    "  def __init__(self,vocab_size,d_model=256,n_heads=4,d_ff=1024,num_enc=3,num_dec=3,max_len=512):\n",
    "    super().__init__()\n",
    "    self.tok_emb = nn.Embedding(vocab_size,d_model)\n",
    "    self.pos_emb = nn.Embedding(max_len,d_model)\n",
    "    self.enc_layers = nn.ModuleList([EncoderLayer(d_model,n_heads,d_ff) for _ in range(num_enc)])\n",
    "    self.dec_layers = nn.ModuleList([DecoderLayer(d_model,n_heads,d_ff) for _ in range(num_dec)])\n",
    "    self.lm_head = nn.Linear(d_model,vocab_size,bias=False)\n",
    "\n",
    "  def forward(self,src_ids,tgt_ids):\n",
    "\n",
    "    def add_pos(x):\n",
    "      b,L = x.shape\n",
    "      pos = torch.arange(L,device=x.device).unsqueeze(0).expand(b,L)\n",
    "      return self.tok_emb(x) + self.pos_emb(pos)\n",
    "\n",
    "    src = add_pos(src_ids)\n",
    "    tgt = add_pos(tgt_ids)\n",
    "\n",
    "    # encoder\n",
    "    mem = src\n",
    "    for layer in self.enc_layers:\n",
    "      mem = layer(mem)\n",
    "\n",
    "    # causal mask for decoder self-attn\n",
    "    L = tgt.size(1)\n",
    "    causal = torch.triu(torch.ones(L,L,device=tgt.device)*float(\"-inf\"),diagonal=1)\n",
    "\n",
    "    out = tgt\n",
    "    for layer in self.dec_layers:\n",
    "      out = layer(out,mem,causal_mask=causal)\n",
    "\n",
    "    logits = self.lm_head(out)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMI8M6_Yv7Qf"
   },
   "source": [
    "â¬ new-model : MiniBARTwithGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5k4aVcg0c9qz"
   },
   "outputs": [],
   "source": [
    "class MiniBARTwithGen(MiniBART):\n",
    "  def generate(self, src_ids, max_len=128, bos_id=BOS_ID, eos_id=EOS_ID):\n",
    "    self.eval()\n",
    "    device = src_ids.device\n",
    "    batch_size = src_ids.size(0)\n",
    "\n",
    "    tgt_ids = torch.full((batch_size,1),bos_id, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "      logits = self(src_ids, tgt_ids)\n",
    "      next_token = logits[:, -1, :].argmax(-1).unsqueeze(-1)\n",
    "      tgt_ids = torch.cat([tgt_ids,next_token],dim=1)\n",
    "      if (next_token == eos_id).all():\n",
    "        break\n",
    "\n",
    "    return tgt_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUT4jgtVwUWN"
   },
   "source": [
    "ðŸ”½  Ù„ÙˆØ¯ ÛŒÚ© Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ Ú©Ù„Ø§Ø³ Ø¬Ø¯ÛŒØ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0H02R4daAK8",
    "outputId": "6ecbb122-e6d6-4eda-f52a-47170bf1abdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBARTwithGen(\n",
       "  (tok_emb): Embedding(50265, 256)\n",
       "  (pos_emb): Embedding(512, 256)\n",
       "  (enc_layers): ModuleList(\n",
       "    (0-2): 3 x EncoderLayer(\n",
       "      (self_attn): SelfAttn(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (lin1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (lin2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dec_layers): ModuleList(\n",
       "    (0-2): 3 x DecoderLayer(\n",
       "      (self_attn): SelfAttn(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (cross_attn): CrossAttn(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (lin1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (lin2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "d_ff = 1024\n",
    "num_enc = 3\n",
    "num_dec = 3\n",
    "\n",
    "model = MiniBARTwithGen(vocab_size, d_model, n_heads, d_ff, num_enc, num_dec)\n",
    "\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Labratory_13_miniBAR_pretrain.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POMpRqj4fZtu",
    "outputId": "5c017692-ca27-4883-fd81-7e582a5d9224"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The company's first-degree murder of the first-degree murder .\n",
      "The company is the second-degree murder in the U.S.\n",
      "The company is the second-degree murder of the U.S.\n",
      "The company is the second\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "sample_txt = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = tokenizer([sample_txt],return_tensors=\"pt\",padding=True,truncation=True)\n",
    "\n",
    "gen_ids = model.generate(inputs[\"input_ids\"],max_len=50)\n",
    "print(tokenizer.decode(gen_ids[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWBD-P54GUpv"
   },
   "source": [
    "## **PRETRAIN-evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc1Mfus3T3sG"
   },
   "source": [
    "#### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GbsviNz8Gg7C"
   },
   "outputs": [],
   "source": [
    "MAX_INPUT = 512\n",
    "MAX_TARGET = 128\n",
    "\n",
    "# Ø¯ÛŒØªØ§Ø³ØªØŒÙ…ØªÙ† Ø®Ø§Ù…\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "test_txt = dataset[\"test\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuweYomLw9zG"
   },
   "source": [
    "#### methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "KLhgab9dJtcG"
   },
   "outputs": [],
   "source": [
    "# Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ\n",
    "def clean_lines(lines):\n",
    "  out = []\n",
    "  for line in lines:\n",
    "    line = line.strip()\n",
    "    if len(line) > 0:\n",
    "      out.append(line)\n",
    "  return out\n",
    "\n",
    "# Ø§Ø¨Ø²Ø§Ø± Ø¬Ù…Ù„Ù‡ Ø¨Ù†Ø¯ÛŒ Ø®ÛŒÙ„ÛŒ Ø³Ø§Ø¯Ù‡\n",
    "def split_sentences(x):\n",
    "  out = []\n",
    "  s = []\n",
    "  for tok in x.split():\n",
    "    s.append(tok)\n",
    "    if tok.endswith(('.','!','?','.\"',\"!'\",\"?'\")):\n",
    "      out.append(\" \".join(s))\n",
    "      s = []\n",
    "  if s:\n",
    "    out.append(\" \".join(s))\n",
    "  return out if out else [x]\n",
    "\n",
    "# Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÛŒØ±ÛŒ Ø·ÙˆÙ„ Ø¨Ø§ ØªÙˆØ²ÛŒØ¹ Ù¾ÙˆØ§Ø³ÙˆÙ†\n",
    "def simple_span_len(lam=3):\n",
    "  L=0\n",
    "  p = math.exp(-lam)\n",
    "  F = p\n",
    "  u = random.random()\n",
    "  while u > F:\n",
    "    L += 1\n",
    "    p *= lam/L\n",
    "    F += p\n",
    "  return max(L,1)\n",
    "\n",
    "\n",
    "def text_infilling_token_ids(ids,mask_ratio=0.3,lam=3):\n",
    "  L = len(ids)\n",
    "  num_to_mask = max(1,int(mask_ratio*L))\n",
    "  masked = ids[:]\n",
    "  covered = set()\n",
    "  while len(covered) < num_to_mask:\n",
    "    start = random.randrange(0,L)\n",
    "    if start in covered:\n",
    "      continue\n",
    "    span_len = simple_span_len(lam)\n",
    "    end = min(L,start+span_len)\n",
    "    for i in range(start,end):\n",
    "      if i not in covered:\n",
    "        masked[i] = MASK_ID\n",
    "        covered.add(i)\n",
    "  return masked\n",
    "\n",
    "def sentence_permute(text):\n",
    "  sents = split_sentences(text)\n",
    "  random.shuffle(sents)\n",
    "  return \" \".join(sents)\n",
    "\n",
    "def apply_noise(text,do_sentperm=True,mask_ratio = 0.3,lam=3):\n",
    "  if do_sentperm:\n",
    "    text = sentence_permute(text)\n",
    "\n",
    "  ids = tokenizer(text,truncation=True,max_length=MAX_INPUT,add_special_tokens=False)[\"input_ids\"]\n",
    "  if len(ids) == 0:\n",
    "    return None\n",
    "\n",
    "  noisy_ids = text_infilling_token_ids(ids,mask_ratio=mask_ratio,lam=lam)\n",
    "\n",
    "  def pack(arr,max_len):\n",
    "    arr = [BOS_ID] + arr[:max_len-2] + [EOS_ID]\n",
    "    attn = [1]*len(arr)\n",
    "    if len(arr) < max_len:\n",
    "      pad_len = max_len - len(arr)\n",
    "      arr += [PAD_ID]*pad_len\n",
    "      attn += [0]*pad_len\n",
    "    return arr, attn\n",
    "\n",
    "  noisy_imp, noisy_attn = pack(noisy_ids,MAX_INPUT)\n",
    "  clean_tgt, _  = pack(ids,MAX_TARGET)\n",
    "\n",
    "  return {\n",
    "      \"input_ids\":noisy_imp,\n",
    "      \"attention_mask\":noisy_attn,\n",
    "      \"labels\":clean_tgt\n",
    "  }\n",
    "\n",
    "\n",
    "def make_tensor_dataset(texts, n_limits=None):\n",
    "  rows = []\n",
    "  count = 0\n",
    "  for t in texts:\n",
    "    ex = apply_noise(t,do_sentperm=True,mask_ratio=0.3,lam=3)\n",
    "    if ex is not None:\n",
    "      rows.append(ex)\n",
    "      count += 1\n",
    "      if n_limits is not None and count >= n_limits:\n",
    "          break\n",
    "\n",
    "  input_ids = torch.tensor([r[\"input_ids\"] for r in rows], dtype=torch.long)\n",
    "  attention_mask = torch.tensor([r[\"attention_mask\"] for r in rows], dtype=torch.long)\n",
    "  labels = torch.tensor([r[\"labels\"] for r in rows], dtype=torch.long)\n",
    "  return {\"input_ids\":input_ids,\"attention_mask\":attention_mask, \"labels\":labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qjTPDRhDScPS"
   },
   "outputs": [],
   "source": [
    "test_ids = make_tensor_dataset(test_txt[:3000],n_limits=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1j1-ge0nVXz2"
   },
   "outputs": [],
   "source": [
    "def to_dataset_obj(d):\n",
    "  return TensorDataset(d[\"input_ids\"],d[\"attention_mask\"],d[\"labels\"])\n",
    "\n",
    "class WrappedSet(torch.utils.data.Dataset):\n",
    "  def __init__(self,d):\n",
    "    self.input_ids = d[\"input_ids\"]\n",
    "    self.attention_mask = d[\"attention_mask\"]\n",
    "    self.labels = d[\"labels\"]\n",
    "\n",
    "  def __len__(self): return self.input_ids.size(0)\n",
    "  def __getitem__(self,idx):\n",
    "    return {\n",
    "        \"input_ids\":self.input_ids[idx],\n",
    "        \"attention_mask\":self.attention_mask[idx],\n",
    "        \"labels\":self.labels[idx]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dSZ15bOwxP-"
   },
   "source": [
    "ðŸ”½ **evaluation-model** ðŸ”½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc0cd0b0",
    "outputId": "aeb877c0-e5a1-47bc-d348-87a9c1ffe95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results on Wikitext test set:\n",
      "{'rouge1': np.float64(0.11217163100231278), 'rouge2': np.float64(0.014261643632793254), 'rougeL': np.float64(0.08903827454187113), 'rougeLsum': np.float64(0.07691927671715536)}\n"
     ]
    }
   ],
   "source": [
    "test_dataset_wiki = WrappedSet(test_ids)\n",
    "test_loader_wiki = DataLoader(\n",
    "    test_dataset_wiki,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "wiki_test_preds = []\n",
    "wiki_test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_wiki:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        generated_ids = model.generate(input_ids, max_len=MAX_TARGET)\n",
    "\n",
    "        # Decode the generated and true labels\n",
    "        preds_batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        labels_batch = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        wiki_test_preds.extend(preds_batch)\n",
    "        wiki_test_labels.extend(labels_batch)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "wiki_results = rouge.compute(\n",
    "    predictions=wiki_test_preds,\n",
    "    references=wiki_test_labels,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"Evaluation results on Wikitext test set:\")\n",
    "print(wiki_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gZO3INSxCTO"
   },
   "source": [
    "ðŸ”½  **eval-results** ðŸ”½\n",
    "\n",
    "| dataset | test_size | rouge1 | rouge2 | rougeL | rougeLsum |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| wikitext | 3000 | 0.1121 | 0.0142 | 0.0890 | 0.0769 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn2Jzy9QGKhA"
   },
   "source": [
    "## **sumarize-evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4ymyIrXxHDQ"
   },
   "source": [
    "ðŸ”½ **Dataset** ðŸ”½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305,
     "referenced_widgets": [
      "ac182ad99d3b44279170242aeda68e8b",
      "518f1e5015ea4bad8321b0ab7e0076c6",
      "91d73ebdb38849e699776dd2764f5ad3",
      "6667d17c8eb94e8cb7c4e265a827d143",
      "b7a656d804584b128b5609ed5a46ce4b",
      "6e758096e3f643d495460b40186acf5b",
      "278ef07762d8498dad4b132ecee73e31",
      "cc4a4902804441be8bf040ddd5b34b2b",
      "6f28975dcb58413db9d8618610a0e53e",
      "c01b6ef0b23b43c9bd3df16a0ce9b053",
      "85a12ab07bdb4c5ca4e1c512762fc261",
      "488a2c63a67546878e9bc5e9e31144a3",
      "315c00ccbacb4597a7c43c5c0b982420",
      "f791ba2b84f74de88bf3ce9cda1b2dab",
      "7a382c38335c48b591d17b99f7c44ea8",
      "f1deac487e8749988f054850dac3bb2b",
      "4fa8e519083e4bda970a7c896ba73f5a",
      "0101b17e516c43aaa56530e907f7a6dd",
      "b84880acbca2425caf202e2a7955c925",
      "95bf20040d3e47e6bc785b6951cd0707",
      "8ca13b6bd888456c8c0cf84b0e95fb3d",
      "b1126c0942bc49ffa04a3c1b469a31d7",
      "3d3a9583ac494970bea9c89eaa620549",
      "6e1632c9176f48c489a478818e31d5c7",
      "e05b45b92c2a46f8b12f360808712d5c",
      "a44ff38679fc4b65a09b4faaa56be2ec",
      "1906e2435c7c4d80946c63d2ba10e68f",
      "29d3dae17fd943d598b9d4544203b4f2",
      "3780327d942442f49e8ca02d07eea5b4",
      "04a20e805d064a19aeaeed426b26e57f",
      "5f4d5f95406b4bacb28004381f389ec0",
      "778e1465749c49d6879913a19d648003",
      "03cf4611e9e3490dab9b6c0ad827af01",
      "7f937f65994147bc919be5210db438b9",
      "c7ae043a0bef48f497c8702ea67c45ac",
      "739f03d8d949442fb8e533268545effa",
      "90d3668873e64957977736d011a833f9",
      "e704109451c64d959a1dc80b535f106a",
      "f074f5c300214cb6a7cf06df76d15638",
      "ecc8bc4d01704afdb702b3a6fcfa738a",
      "247805301b9a46a5b8197dd5fd79b379",
      "10fa85640c194c0b9ff773f3d2cc7bf9",
      "5de14c3f26e04f1fb74caaa09d555a1c",
      "8f3a39daefcd4592a1d691b6a3fa7539",
      "97c01ddf5b614b7489cdf0751ab94bc9",
      "8415f145303b483fafc47fb4fce4a8a9",
      "fd2a8d93fd8045a2a22605c0930c2cc6",
      "b3c93b826b6a47e78d84548b8dbfc3d3",
      "68caa66d113f477ebd25fb59fd6418c3",
      "148b474848584310927bf9b1392bf4d1",
      "41133ddfb4c841f7836f37728a33b444",
      "23fd015ee2994c2181b32025f97215ba",
      "75f6ea36de63464eb87f090eee171385",
      "114c900cfe7948b8bf9ac4119d7a2536",
      "fd2786501e6344ab80f87fcb5db66a8c",
      "a264e8f14ca04dfa842ff58171bb6276",
      "b699b519376e4858bc31c62692e7d2e9",
      "5ec247aacb684613bf6e9f9e9dc85c33",
      "018d0813f85f4fe4be7c032a534adce9",
      "ebd3e669c68c4f4082b49f9288aa8c23",
      "19c4fdc6905241efb4857180104d61fc",
      "6ffcfffb216b47e6944c680658e10257",
      "018d635317d6494aaada981d9e112d8a",
      "8171207d8c3e420884d093f69be79340",
      "7830b6bbcd6e457d9e0c872649560b23",
      "c7337c9c5ebd485d91734d938b5e428e",
      "f09de25fc4ae47239acb966f2e5aaf3c",
      "b90556740a694a8283655dadd2c37bcc",
      "006e6979f23146bd95a84631f2de5c5d",
      "5406b8ee2be54a659ea3b9419a2ee3f7",
      "028ab4f21d64497ca65fd20c9c3c0b9e",
      "2f62240b757b43f797d6b1ef1119f928",
      "dbfeb6e110694d49bd2b8f3e02a463c0",
      "db392fef07b146feafd298bcc4bd91bd",
      "62d118b777bd460c845cc8e5cfafbd4b",
      "8f62a8d578224f9cb25b967bd576c611",
      "0ac54294ec044cd8a084773306c24c23",
      "dad2512b7d5f49fbae9c675b39492eee",
      "1ec4c773b6564c408b3f5be3d6fe5e5e",
      "731451cb050c478cac2111b5a0c187d3",
      "143186d96fbb45348eb9e62fcbc82507",
      "cff5e6648e3a40c6a07a32e7d7044e51",
      "f43b23f3e7b241f9b961f745bb72cb21",
      "2459b8c8ae774c2e84a89f9861987986",
      "1594f09fc18e4106b7098d9eefcccd89",
      "69288f31acc94c33b00ea519f10ca901",
      "ec9f64628c514409a46905e418c29d2c",
      "67de38c1adc64b9998cabd5b4281fda9",
      "009fa120798944dfa5ef17a25c9a3b3a",
      "83a54fd9fcbe433d88312c9ea26e63b2",
      "b65848df7b394911930150062c25089c",
      "e45dfe68963c4640b21c7fddb4c26001",
      "3d25034306c5429485e2f8e83e5048b9",
      "39f24178e5904b38bd4961aff9a7e79e",
      "604bf0f90b2245279034051543f437cc",
      "a5247a5f40de40e09369b5fbf4ee3c76",
      "8cf3ba1ad2a047e7a77d13c5d78f4aa4",
      "abd07f5461ee486db7ff53d71a0329c3",
      "b91209550dcf43bc984f9f5fabfccdff"
     ]
    },
    "collapsed": true,
    "id": "kMj06JHIxLzr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c6c0096c-8f6d-4367-dccb-e2968707b17f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac182ad99d3b44279170242aeda68e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488a2c63a67546878e9bc5e9e31144a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3a9583ac494970bea9c89eaa620549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f937f65994147bc919be5210db438b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c01ddf5b614b7489cdf0751ab94bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a264e8f14ca04dfa842ff58171bb6276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09de25fc4ae47239acb966f2e5aaf3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad2512b7d5f49fbae9c675b39492eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009fa120798944dfa5ef17a25c9a3b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_dmail = load_dataset(\"cnn_dailymail\",\"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "U4dQ6alZxaY-"
   },
   "outputs": [],
   "source": [
    "MAX_INPUT = 512\n",
    "MAX_TARGET = 128\n",
    "\n",
    "def prepprocess(batch):\n",
    "  inputs = tokenizer(\n",
    "      batch[\"article\"],max_length=MAX_INPUT,padding=\"max_length\",truncation=True\n",
    "  )\n",
    "  targets = tokenizer(\n",
    "      batch[\"highlights\"],max_length=MAX_TARGET,padding=\"max_length\",truncation=True\n",
    "  )\n",
    "  inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE1uy-x08Ooy"
   },
   "source": [
    "### **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2d5c4c9108f141fcba4ee94025808da3",
      "2e3fe6ee529f436782a920ebca8cc335",
      "8a4567fa09ac43ebaeecc0db5f6ace2b",
      "55cef632176244c18ad1db45f07acd39",
      "f0fdb9a6f7b54c4383e7c3c83e89a3d6",
      "a7332e39d6de4976b2813895dcad6b0e",
      "bc0dc773d55b492f819d1ce7c063a130",
      "64d74569ee8b4d5bbd37d3e389a8e074",
      "a011a493037d47f5a09a36469ed6e129",
      "34e676a220ec48669fbb2a6a96970935",
      "fc996caf9d4242908e440c93295dc091"
     ]
    },
    "id": "60fa65d6",
    "outputId": "3527f0bd-1a4f-4f53-e140-244b931da255"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5c4c9108f141fcba4ee94025808da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_cnn_dmail = cnn_dmail[\"test\"].select(range(2000))\n",
    "tokenized_test_cnn_dmail = test_cnn_dmail.map(\n",
    "    prepprocess,\n",
    "    batched=True,\n",
    "    remove_columns=test_cnn_dmail.column_names\n",
    ")\n",
    "test_cnn_dmail = tokenized_test_cnn_dmail.with_format(\"torch\")\n",
    "\n",
    "test_loader_cnn_dmail = DataLoader(\n",
    "    test_cnn_dmail,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMfj7M-8y6vI"
   },
   "source": [
    "ðŸ”½ **evaluation** ðŸ”½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0II1DivZi6Ir",
    "outputId": "ee8872c7-00df-46c3-e980-c7bccf8215ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.09341155441423496), 'rouge2': np.float64(0.008701909700815441), 'rougeL': np.float64(0.07751997793752094), 'rougeLsum': np.float64(0.08933795377417134)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_cnn_dmail:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Use the generate method of the MiniBARTwithGen model\n",
    "        generated_ids = model.generate(input_ids, max_len=MAX_TARGET)\n",
    "\n",
    "        # Decode the generated and true labels\n",
    "        preds_batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        labels_batch = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        test_preds.extend(preds_batch)\n",
    "        test_labels.extend(labels_batch)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(\n",
    "    predictions=test_preds,\n",
    "    references=test_labels,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nGknaSxyNsL"
   },
   "source": [
    "ðŸ”½  **eval-results** ðŸ”½\n",
    "\n",
    "| dataset | test_size | rouge1 | rouge2 | rougeL | rougeLsum |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| cnn/dailymail | 2000 | 0.0934 | 0.0087 | 0.0775 | 0.0893 |\n"
   ]
  }
 ],
 "metadata": {
  "cells": [],
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.10.12"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
