{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3BsCS6Cvkqb",
    "outputId": "cb4d3897-6122-414f-e78c-11e79168c689"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oRVQcqz__kZm"
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8M0WGWZz2dg_"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torchmetrics import Accuracy, F1Score, Precision, Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0xcR-q_78tf"
   },
   "source": [
    "## **1.Model_Components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd31gtYAoMFH"
   },
   "source": [
    "##### **SelfAttention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LuLkpNTH8JCQ"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, n_embd: int, seq_len: int, attn_pdrop: float= 0.0, resid_pdrop: float= 0.0):\n",
    "    super().__init__(self)\n",
    "    self.key = nn.Linear(n_embd, n_embd, bias= False)\n",
    "    self.query = nn.Linear(n_embd, n_embd, bias= False)\n",
    "    self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "    self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "    self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "    self.register_buffer('mask',torch.tril(torch.ones(seq_len,seq_len)).view(1,1,seq_len,seq_len))\n",
    "    self.n_embd = n_embd\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C = x.size()\n",
    "    k = self.key(x)\n",
    "    q = self.query(x)\n",
    "    v = self.value(x)\n",
    "    att = (q @ k.transpose(-2,-1)) / math.sqrt(C)\n",
    "    att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "    att = F.softmax(att,dim=-1)\n",
    "    att = self.attn_drop(att)\n",
    "    y = att @ v\n",
    "    y = self.resid_drop(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNFSzCYpoEaZ"
   },
   "source": [
    "##### **MultiHeadAttention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6vfC7xA_-8Ts"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self,n_embd:int,n_head:int,seq_len:int,attn_pdrop:float=0.0,resid_pdrop:float=0.0):\n",
    "    super().__init__()\n",
    "    assert n_embd % n_head == 0\n",
    "    self.n_head = n_head\n",
    "    self.head_dim = n_embd // n_head\n",
    "    # concatenated attention weights : (B, T, n_embd) -> (B, T, 3 * n_embd)\n",
    "    self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias = False)\n",
    "    self.out_proj = nn.Linear(n_embd, n_embd, bias = False)\n",
    "    self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "    self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "    self.register_buffer('mask',torch.tril(torch.ones(seq_len,seq_len)).view(1,1,seq_len,seq_len))\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    qkv = self.c_attn(x)   # Concat\n",
    "    q,k,v = qkv.split(C, dim = 2)  # dim = 2 -> تقسیم میشود C بعد سوم به تکه هایی به اندازه\n",
    "    q = q.view(B,T,self.n_head, C // self.n_head).transpose(1,2)\n",
    "    k = k.view(B,T,self.n_head, C // self.n_head).transpose(1,2)\n",
    "    v = v.view(B,T,self.n_head, C // self.n_head).transpose(1,2)\n",
    "    att = (q @ k.transpose(-2,-1)) / math.sqrt(C//self.n_head)\n",
    "    att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "    att = F.softmax(att, dim = -1)\n",
    "    att = self.attn_drop(att)\n",
    "    y = att @ v\n",
    "    y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "    y = self.resid_drop(self.out_proj(y))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hldUbbdUn9-X"
   },
   "source": [
    "##### **TransformerBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EWcGJSQDP703"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, n_embd: int, n_head: int, seq_len: int, mlp_ratio = 4.0, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.attn = MultiHeadAttention(n_embd,n_head,seq_len,attn_pdrop,resid_pdrop)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(n_embd,int(mlp_ratio * n_embd)),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(int(mlp_ratio * n_embd) ,n_embd),\n",
    "        nn.Dropout(resid_pdrop)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x + self.attn(self.ln1(x))\n",
    "    x = x + self.mlp(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mjLMV8Onp2j"
   },
   "source": [
    "##### **ConvBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x36PkfCISBRv"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, n_embd: int, pdrop= 0.0):\n",
    "    super().__init__()\n",
    "    self.ln = nn.LayerNorm(n_embd)\n",
    "    self.conv3 = nn.Conv1d(n_embd,n_embd,kernel_size=3,padding=1,groups=1)\n",
    "    self.conv5 = nn.Conv1d(n_embd,n_embd,kernel_size=5,padding=2,groups=1)\n",
    "    self.proj = nn.Linear(2*n_embd,n_embd)\n",
    "    self.drop = nn.Dropout(pdrop)\n",
    "    self.act = nn.GELU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    h = self.ln(x)\n",
    "    h = h.transpose(1,2)\n",
    "    y3 = self.conv3(h)\n",
    "    y5 = self.conv5(h)\n",
    "    y = torch.cat([y3,y5],dim=1).transpose(1,2)\n",
    "    y = self.proj(self.act(y))\n",
    "    y = self.drop(y)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4eerMGGpS3r"
   },
   "source": [
    "##### **MLPBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JQNMKU7LpRtP"
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "  def __init__(self,n_embd:int,mlp_ratio=4.0,pdrop=0.0):\n",
    "    super().__init__()\n",
    "    self.ln = nn.LayerNorm(n_embd)\n",
    "    self.fc1 = nn.Linear(n_embd,int(mlp_ratio * n_embd))\n",
    "    self.fc2 = nn.Linear(int(mlp_ratio * n_embd),n_embd)\n",
    "    self.drop = nn.Dropout(pdrop)\n",
    "    self.act = nn.GELU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    h = self.ln(x)\n",
    "    h = self.fc2(self.act(self.fc1(h)))\n",
    "    h = self.drop(h)\n",
    "    return x+h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VtMjQDvtMot"
   },
   "source": [
    "##### **MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hpkyt_W0tTSA"
   },
   "outputs": [],
   "source": [
    "class HybridLM(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 10, \"This script assumes 10 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "        ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "        ConvBlock(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "        MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "        MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p07IQRNBujTx"
   },
   "source": [
    "## **2.Dataset-Wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ByZQB-hlu7w-"
   },
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "  def __init__(self,data_ids,seq_len):\n",
    "    self.data = data_ids\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) // self.seq_len\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    start = idx * self.seq_len\n",
    "    x = self.data[start : start + self.seq_len]\n",
    "    y = self.data[start + 1 : start + self.seq_len + 1]\n",
    "    return x, y # Corrected to return x and y separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kJLJR1dxMTu"
   },
   "source": [
    "## **3.Lightning-Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "6Wfse24-nS0g",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LitHybridLM_Models(pl.LightningModule):\n",
    "  def __init__(self,ModelClass,vocab_size,seq_len=256,num_classes=4,n_layer=10,n_head=6,n_embd=384,lr=3e-4):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.model = ModelClass(vocab_size, seq_len, n_layer, n_head, n_embd)\n",
    "    self.lr = lr\n",
    "    self.perplexity = Perplexity(ignore_index=-100)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    logits, loss = self.model(x,y)\n",
    "    self.log(\"train_loss\", loss, prog_bar=True)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    x,y = batch\n",
    "    logits,loss = self.model(x,y)\n",
    "    ppl = self.perplexity(logits,y)\n",
    "\n",
    "    self.log(\"val_loss\",loss,prog_bar=True)\n",
    "    self.log(\"val_perplexity\",ppl)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.AdamW(self.parameters(),lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET1vhvsVRnDu"
   },
   "source": [
    "## **4. Load Dataset & Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525,
     "referenced_widgets": [
      "d2a5ca78fed34b48912911e4ea6dded6",
      "fb8e56dd97bf4979ab2ef11c7aee8415",
      "0c7ccdfe8e1a45d08e75f89821209abe",
      "70cdbadb35994096a5696d0ece96ad3a",
      "6381e418ff054a4ca68836fadf25fe85",
      "b6c5d1b652af48e48e6a478de918f6fb",
      "358c173303d041b783ebaaca0dbc8562",
      "8f23cf640ea24bbbb3c82d2d8e221a61",
      "b9b4cbcc77244a139f8be527f34862e8",
      "dae26c928a5b42b2a305a42f2792e88e",
      "c1db7d694774492f84e8ad5dc35a4a67",
      "3220eb90ed344cffaaed211e8a9709b8",
      "0e68b0aabe134fc1b2dcd86a26d937be",
      "f3a39247a39145d3a8a6a5644bc659f2",
      "11e2d5a11a5f43159bacfb8ff0b627cc",
      "48c4f7057adb4badb20f8decf874dadd",
      "f319337ee3cf4c9182cc2911faf011ca",
      "2977a09072f54c0588b20dafa31f71c2",
      "c59283a5331c4589bc445be79532cfb6",
      "cc2df85c02ba4cbda381b37c89f03a86",
      "b7211595a7fc41468f042f0ca9f345ca",
      "9818931816c44c0f8d075709ea76f8bc",
      "41ce9faa65874519b4311c18bc75611b",
      "c86b795a976c4e3b952213597e93890f",
      "70007055ebbf4d7588406bc0975dd25a",
      "d335b44a066848c6abcbf28e0aad0f27",
      "5aa1550e7c2d4dc5982b5e07098b2c37",
      "73fe7f40b29e401c9a2d151dbacc1228",
      "9b550bbd5f984360b51a8af716e3967c",
      "f7bf2695e3444cb097acf2156f1514d8",
      "7ca3d27c51b44417abda1d12bb88a8c3",
      "3d50ec1dced94b0a8942ab7f5dc2937e",
      "2835b3a3253b4bd9912711c09d57fe16",
      "bbe21714f8b24226977b434d7f4c94de",
      "67edbc321db74437be095a7da89dfbfe",
      "383e8cbd902e416d8d3d35be17f67299",
      "bd28fc307d4d4d34bf817c9d82c71135",
      "de907200b3a7446ab4b12d8240a477f6",
      "fc8bf6a9f1a744de9d5076fae20f77bd",
      "0654153de19d4b06a469202e8a1a471e",
      "e28427bfd0e444768f1c7007b64a2ee3",
      "9db83ac04e944256854080f4ad0ccd3b",
      "c41c152721544924be4a2d8ed1ff06cf",
      "7b5621a81d614a10a9e9b6ea20d5e7f3",
      "399c1e9a18d1486aacc568d4e30a4a5a",
      "23b3b0a2e53d405fb7a1146b8e5249e7",
      "e98bfeed6de045fe9e4c03092d537b20",
      "8f7b5bbee30e468995d66d9ebb3ab367",
      "03134eed01814c0399948fe6e5e6fd5e",
      "d92147b06ef649b4af0a1d71c7542124",
      "2f2d1012a86a4278b0ce7428c7dd12ea",
      "8e61697ef7ee403a80ca8b3407ea1ef4",
      "ebcf8624adf047649d0926eaa63a417b",
      "13f50ba96fc449e1b36f29d63c8d6a7d",
      "0c893b3e2b594b97a666b2d00c4710df",
      "78016dd9f92f4e7ca729ed488c60a04c",
      "3d3fd1670875401984ef198d887822c3",
      "4d53c25fe61948e3b2d5aa25f6ec31a7",
      "61db8de256b44ac28346727aa72e78cf",
      "63ebd5fc6da04ef992acafcefa11222e",
      "bced1524e7a241ca8fba6cace9c4cc45",
      "ca1bc72d8e964473a5878a92907d573b",
      "f44333b01d07424caf5b71fd7310e17a",
      "5610a4f6feee4e04b7cd3788fb13fa05",
      "0270118c48d74e1992bca406490321b5",
      "55c5088749b74f9ab2af723c69cc33b0",
      "79e64e6b65b5432eaac8a35ccf74c714",
      "45fab4d0fe9e490695d897c96a90fd50",
      "77f0a2dc8a4547928538a345e71e7326",
      "b9edebe2cb1940bca9f888c0a99318db",
      "5bfb37ae4c944ccdade9aaf0092be425",
      "29b22d9d04fb471b93ddcee4a6c56c1e",
      "18c048a3a0bf4d3aa87a4fd2bccdd511",
      "af3703475a854072a6b505c5513299cd",
      "2d230531d0ed42cf80e97312f859e1df",
      "4a7ca4e9ca344018942f25d2a9329fbb",
      "515742bfd0264b9cbb60934914494eef",
      "6b1687a816df416e965fe60fe97328d0",
      "c122f05ee43c4ad894e09c5284304d14",
      "cfd5aa2a6cf54895a116992e5aa3b6dd",
      "64ed0852281b4d7095035c046b6d85a8",
      "df4896caa5d74ea4bfb7486a630923ba",
      "f81400c9e39f48199e58b6e42122f0be",
      "c6a73e3a114440c58992aeb63e3ab16f",
      "b370f8f0d57945848b1d94a90ed46557",
      "540d014196c7429286f6534cd7bfcfa8",
      "c051666b37ac4ec68a8ddf16dac8f192",
      "36ce961395124015bcccd452a2e0511c",
      "0e7174152bf647c19069def4f2b3d742",
      "7229c7aa219f4b80bcf370a73d00de0b",
      "708a33b74c6d40e8bc780e10c4a501e7",
      "bc42ecbf4274466ab570506fe38331de",
      "e2ff1052eb764fd8882eb12d702fd79c",
      "4fab2314f92f4fde910fb4841cbca322",
      "3f55611b45ff461bb7f2e50db300ea87",
      "755e3f0329dc41b7917ba0da0801f898",
      "224ea48e057e4ec8bcdfd217366d3e19",
      "ab70dbec2956412a9f0164464aa4124e",
      "f62958d65edb41c2924d3ead905c10fa",
      "219e9d1a9bb54ebc98b8c861e88d4d4a",
      "f82664719acf4e9384bd2d35e00f8e82",
      "a76af9f6714044349f86478ccf3ec156",
      "3e9e56d75c564d2dbc2aa536971f3d5c",
      "60bdb20f602f4ba9874f0f9bc49a72db",
      "8aa1681a3b404b88bba4e6a2698d71f5",
      "4f0eab3429ae452d9ecbb95770077809",
      "017820ab2fc54d66b76a8e64c25992da",
      "a9bfeb919a504b3aaf0a6389a2cd3390",
      "1cc6fea7f3264be98b4bf66a4624609e",
      "3fbeddc61d074277b4620304e08c0dca",
      "9b0f3a3c7a5741d787a0d254d68ec15d",
      "55b3845b1e604118bf25964abe315ef5",
      "b1a3123402204ec6a4c53fdc97cd0d31",
      "95c1ac1c72cf4870855ac0db729c279a",
      "5d36f9e0c4844130b57a084d1c1782c3",
      "e6372ffad78543a9a3f1566e36874457",
      "c4c900aa613149cca4ab0ac405e17734",
      "0db51b1947c846658c714049c80ab71d",
      "e2da9383b95745a8ad4a6e24cb7b9fc6",
      "813f01b6ee5a4c3da20c7c9f382e1f8b",
      "d325944d014644b6b191d6336122a95c",
      "2fa04fcbd3bc4a93968940774985228b",
      "03e1d7d95ee24188a9bb0681e9ddf87f",
      "1bfac50a73484a42952746320a7a7c04",
      "a7b2506aab074bd699986369c20fe9b0",
      "ba01867f925445ca81616b09f553565e",
      "b03fd55fa7b24751855f5610c86d8f22",
      "3060c2157383428ca928f4129c78a1aa",
      "10844653e67548278ee7d0e1010715af",
      "4b162aa8a90a40b59c82f1099259fbd2",
      "78840397850447ce84169f5eb0fd6852",
      "a13fb8d1180f4c3da6c11c7e788d9a64"
     ]
    },
    "collapsed": true,
    "id": "nazVhK6LQ5Ab",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3c144ae4-9882-4795-8885-65f26515b103"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a5ca78fed34b48912911e4ea6dded6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3220eb90ed344cffaaed211e8a9709b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/6.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ce9faa65874519b4311c18bc75611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/641k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe21714f8b24226977b434d7f4c94de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/713k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399c1e9a18d1486aacc568d4e30a4a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78016dd9f92f4e7ca729ed488c60a04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e64e6b65b5432eaac8a35ccf74c714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1687a816df416e965fe60fe97328d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7174152bf647c19069def4f2b3d742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219e9d1a9bb54ebc98b8c861e88d4d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0f3a3c7a5741d787a0d254d68ec15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa04fcbd3bc4a93968940774985228b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"mikasenghaas/wikitext-2\")\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdNHVoB5yxRl"
   },
   "source": [
    "## **5.Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ShUyddmBzHnP"
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "  enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "  return enc[\"input_ids\"].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JcotxR6hzKxg"
   },
   "outputs": [],
   "source": [
    "def data_loader(train_size:int,val_size:int,seq_len:int = 256 , batch_size:int=24):\n",
    "  train_texts = dataset[\"train\"][\"text\"][:train_size]\n",
    "  val_texts = dataset[\"test\"][\"text\"][:val_size]\n",
    "\n",
    "  train_ids = encode_texts(train_texts)\n",
    "  val_ids = encode_texts(val_texts)\n",
    "\n",
    "  #seq_len = 256\n",
    "  #batch_size = 24\n",
    "  train_ds = TokenDataset(train_ids, seq_len = seq_len)\n",
    "  val_ds = TokenDataset(val_ids, seq_len = seq_len)\n",
    "\n",
    "  train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "  val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "  return train_loader,val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0Dm2Fo020lih"
   },
   "outputs": [],
   "source": [
    "def train_method(ModelClass,train_size:int,val_size:int,n_layer:int,seq_len:int=256,batch_size:int = 24):\n",
    "  train_loader,val_loader = data_loader(train_size,val_size,seq_len,batch_size)\n",
    "\n",
    "  lit_model = LitHybridLM_Models(ModelClass= ModelClass,vocab_size=len(tokenizer),n_layer=n_layer)\n",
    "\n",
    "  trainer = pl.Trainer(\n",
    "      max_epochs=3,\n",
    "      accelerator=\"auto\",\n",
    "      devices=1,\n",
    "      precision=16,\n",
    "      log_every_n_steps=10\n",
    "  )\n",
    "\n",
    "  trainer.fit(lit_model, train_loader, val_loader)\n",
    "  return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgAUP0LzBCBq"
   },
   "source": [
    "## **6.Train**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532,
     "referenced_widgets": [
      "32d9be64efd54886b0279098ed4ab01f",
      "d9c3deb990ec4afabfc863c5f2aaab9f",
      "b1447b4974e949588e01a8f82ae2cd0b",
      "bae85bf3b0954e6e9fc80f8c8f336e53",
      "350659b50f934d2abce99ed77829693a",
      "a078569da39b4b12bd88dfcb06b61fc3",
      "2bdc232102bd4c35b00ab6e7ecfb36cf",
      "c17042a161934dd4970215bcdf296b3f",
      "de6a9ed135a74c84a81e9abb523632fc",
      "b4613f7e31c34ab98b98b120ae3a4ab1",
      "649344d4384746f3a5490bc7e5284a1a",
      "4bca8814f1ec48ac80e8ad2344f0cb35",
      "69723d4f82544855b95c6e21d056e817",
      "04c0fa72f22f402bb239ed596f132853",
      "30528c77ccdc4aa8b022b228ab97641b",
      "ed4c2d689753446094d179ec0d2cc09f",
      "40bd719e7c224cbcab663fc4c55b7bc5",
      "e86a4a8497ce43f3a36ad21907214778",
      "24d85e06d1774e509d26519515e1ca86",
      "c464f237f65347729a190fa4314c4acd",
      "03326632fffc48f0a7d42e8c1ee280a9",
      "89c15ce364fa4845ae4ea060e2957d9b",
      "eb45b90652134e74a5e5210f41f1f3be",
      "85df9a96c2224d00bbc05fdab8367191",
      "731bd5d6af3b439482137767b9e85dba",
      "84efbd2f3a6d48b793f6efd4c3344463",
      "2e333d05adf041d782acedb29aeb1354",
      "0271dcfd6a9940f48af3cedc8328a457",
      "b9cfcf4690b54d72946ce250e88e3b87",
      "1bcd7ae919214b08a00f550b68e7e535",
      "d8a83fa4a5c649a0bf54701efec2f8c0",
      "211e206485bf4d83be726f72bee29873",
      "85c70eda960544219e057d8fcd589f3a",
      "f8844d33f5ac473ebd5967788461292e",
      "0481533c07ef4eb78f121ad42e68d810",
      "1af3246174554705b450f581229789f9",
      "9b2f27ffdc2e4c27af260cf2834b460f",
      "f64ab2606ec24d22b3630accfedfea0a",
      "c928b91d98c64700a8111f377b0b59f0",
      "051ddaeaa9db41b8953f6efd877db385",
      "3e25aeef26da41c0b59b1583973d9625",
      "52642519f9d2408690e5c2afb3c77eba",
      "617481bd96fd46b8ad5998e7d0dbc3d3",
      "5f12cf280301416ba7996b01e4e9af71",
      "f8732f3b84a142a593294b02233f4f37",
      "517088e632964fb696c8d828cb470430",
      "ce39142bd94547578d2a1f430617208b",
      "9ff46bc72608450fa59ae4a39d9d981e",
      "2bf493634db14c5e93bcdd54c7882cfc",
      "7e41974ef80b4bfd9bdf288a8f10438b",
      "a649d54d0fae4c9189928d660168bec0",
      "2af26f5fc5c64efabd51a76f20e1bfad",
      "116c66c08cce4fb5a4115a906e4610aa",
      "ebd55072e6b049ce815d7b17434107a8",
      "fce13216e4a44ce48b8ed4bf9bdb7f38"
     ]
    },
    "id": "4S8-zcvzsQKD",
    "outputId": "c6afce0a-9c93-4b1f-ce1a-051d3b3c4c31"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `Perplexity` from `torchmetrics` was deprecated and will be removed in 2.0. Import `Perplexity` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/usr/local/lib/python3.12/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM    | 54.7 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "54.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "54.7 M    Total params\n",
      "218.607   Total estimated model params size (MB)\n",
      "111       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d9be64efd54886b0279098ed4ab01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bca8814f1ec48ac80e8ad2344f0cb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb45b90652134e74a5e5210f41f1f3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8844d33f5ac473ebd5967788461292e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8732f3b84a142a593294b02233f4f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = train_method(ModelClass= HybridLM,train_size=30000,val_size=5000,n_layer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eefnctV5YlsZ",
    "outputId": "7fecd680-5771-4db2-af26-5a9e7b522c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0242 | Perplexity : 1.02\n"
     ]
    }
   ],
   "source": [
    "val_perp = trainer.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss:.4f} | Perplexity : {val_perp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOY16qFdSGFA"
   },
   "source": [
    "## **7.Labratory(3)**\n",
    "\n",
    "Dataset : __\"mikasenghaas/wikitext-2\"__\n",
    "\n",
    "Changing the model architecture,\n",
    "Changing train_size and val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xE-cjwuTxY3"
   },
   "source": [
    "### **Train(14)**\n",
    "\n",
    "model ( 1 Layers ) :    MLPBlock\n",
    "\n",
    "train_size : 30000  \n",
    "\n",
    "val_size : 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqw02XfeTxY4"
   },
   "outputs": [],
   "source": [
    "class HybridLM_14(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 1, \"This script assumes 2 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "        MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "     #   MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "     #   TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596,
     "referenced_widgets": [
      "84fd642ab27f4a08968a27d24c0f5b78",
      "7dd21a6d57704dc6b76b481f52f4a174",
      "b8a807ca3c1848efb888aeac9c8e3379",
      "f605838703644c9783c009351476579f",
      "a23f33a8961e4e198ed88ed4d4ae9ebf",
      "ca14a3d826ba4c32ad6f014e16e71334",
      "09e0d98baeb143feb5a011ad09d15fe2",
      "0b1e11a9d8e44585929158865ad54dc7",
      "7bf7ace1d33442dba03d6fef17f61b90",
      "203eed6b4261418383db3aaa3b7893e6",
      "049fe58a3a7e4bb8b319bfce45d7c8e1",
      "63e40dae9a754576a6311bb56e354dbe",
      "c20397dfbf8c4f3f8f32c452e8a7d845",
      "a07411a9b7944c0fa06cae9a8e9943c4",
      "d257d025189648c594f6b89990f4c58e",
      "fcb69147ac14445a99edc0bcef01d2f8",
      "b22b0f4fa57a45819bbaad25e18d5e5d",
      "986b598696fa4b4983842b67e4a44403",
      "6dd194419be947eba489c6aaba2bdfb0",
      "1327e7be6862441f991b89cc6f25322e",
      "49b5a1c8d3fc42f0bd0f65836914fe64",
      "fb8605f821e642ddb386b4f03c340bd0",
      "e787daea729f42ad840c5a1142af52d9",
      "0b4adf78e86d4b709c82b4f74af95d9a",
      "f273eef594a5463092c7cf56e7b965ed",
      "4ca3cf7fddfe4ddfbcec165cb91ed0e5",
      "ec5b26a157074e7d9f53b7caf666b6cf",
      "6d0dca557b914f5a8f9b76fcda9d93a3",
      "dfb692d63b1a43e2920e7fb1c7167edb",
      "68ed91342cf14c15badd8f7eb82db874",
      "abd84c9d58784fa5bfb528423a98dc46",
      "f8a15d6ca17d4df1a15b8734ad1a8b92",
      "605e8f615aaa4f7da076d5e5e164bf0a",
      "4c957daedb7d45e9adbb433bb57857ef",
      "e7753abc5f794d78bb6f2a23aa0ac131",
      "5280393dce584c2ba4d2ada6a680ca9f",
      "d6c2110ad1af40af9b971554d9a40a83",
      "3ef3f175cfca43e2a490ee7aaca666cd",
      "ba0bc1c0ce0d48b589e4d1a9b97b16bf",
      "800525b1786542169a988c8303a0c59f",
      "6ece71730c174acc9bf4bb1828f2ea87",
      "071963e01bf54b17918cc3dac075bddc",
      "6f86d6b20adb4f1596b78fecb7f10b12",
      "7986721aa3384cceb916f6ad46a974af",
      "cf8e67ece6194187bc7136bec7f66034",
      "7e73afb83b48433a930c801dcddfd5c7",
      "99e0ed7673674a3bad060067da00667b",
      "46d9223c665749748ca88f1c6abeb819",
      "f70bbb86cb7045878955cdd3a35a94bd",
      "292782f704d84c8ebd8b3628f826a47d",
      "0ce1863327ec47d8abdbc62cf7d07a26",
      "c3e0e322c6ee492c9b71f20c4221bf24",
      "97f36e4650db48f7873e87f0b79b1e16",
      "2eef145771b44ad8b8f461a41081cdcf",
      "3e3b4f5bbfaa4079af7b4d60fe5dd251"
     ]
    },
    "id": "mIyxudTPTxY5",
    "outputId": "a6789640-b6fb-4618-e232-5f483ade17b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `Perplexity` from `torchmetrics` was deprecated and will be removed in 2.0. Import `Perplexity` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/usr/local/lib/python3.12/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM_14 | 39.9 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "39.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "39.9 M    Total params\n",
      "159.515   Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fd642ab27f4a08968a27d24c0f5b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e40dae9a754576a6311bb56e354dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e787daea729f42ad840c5a1142af52d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c957daedb7d45e9adbb433bb57857ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8e67ece6194187bc7136bec7f66034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_14 = train_method(ModelClass=HybridLM_14,train_size=30000,val_size=5000,n_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eF6jO9k7TxY5",
    "outputId": "ac24d51b-b8ef-4485-f753-c3588a236e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 1.4109 | Perplexity : 5.15\n"
     ]
    }
   ],
   "source": [
    "val_perp_14 = trainer_14.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss_14 = trainer_14.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss_14:.4f} | Perplexity : {val_perp_14:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cw_L6n5GhwXk"
   },
   "source": [
    "### **Train(15)**\n",
    "\n",
    "model ( 5 Layers ) : ConvBlock -> TransformerBlock ( * 3 ) -> MLPBlock\n",
    "\n",
    "train_size : 30000\n",
    "\n",
    "val_size : 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04wx33nxhwXl"
   },
   "outputs": [],
   "source": [
    "class HybridLM_15(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 5, \"This script assumes 5 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "        ConvBlock(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "     #   MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "        MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "     #   TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451,
     "referenced_widgets": [
      "e83534e0451749688f419dbadbb3b901",
      "3ebff6a8ac8843489d1145ab4462daed",
      "2adf79ae8b5047f28757e92be8046c6e",
      "beed6762dbad4a40a89581ba0c3eaf68",
      "0ccf8342e78f4c10afdec4c242684b6f",
      "9458d148abf54ed5bba0636eb8ba271c",
      "65d944e532a5495e93379ed641692afc",
      "194d792123b445a1baeb9dfa8d1983c9",
      "93359dabb74049d2a9d649ae8d01bd61",
      "070efcb9f3af40a3ab35b84f21aef2c5",
      "347fc1a3f54e45bbb532c33fcef796fc",
      "688b298126b14ef3977f6181f513e7db",
      "d0220d86cf0148aab7cf7274bfec3131",
      "b0227bed356f4675b3c0179d214951b8",
      "78eea3d5d2704f7490973c0385617915",
      "5029ff1c949f4378a111bfcd6b0c6beb",
      "94036ce880e9489f8e66c67af0b5fceb",
      "baf7b1d0ec7f429d836499008fa3518b",
      "a97e65eb0c87491e8137dd30d153a895",
      "837d3bdd03594bcdba3425619a30414a",
      "4971b74319fd4190bd62e8cd4454af5f",
      "a080a46d09ed4c079466b791d652d6e5",
      "2f1ade6aa15742409fb417583e0a624b",
      "b2326b0138c74f09a045d1388ef3de4d",
      "e9d4f2624c8146cb957991d00cbfe90f",
      "21da1a0cb7674185b8125366e4a5c3b8",
      "63567ebc71f846f4a785e590455547d9",
      "162488cfeb9b4ac88c71372909eeaeea",
      "26d8f0022d824a17aa3730735fed4d65",
      "8434a6ad81ae4524ab43a87fc0b928c8",
      "c0f129fc59d544cda9bf0bb1de0fc674",
      "d473fdc5f6fc485884862a72bcdd7e47",
      "dff94f2b22134c069633709682747768",
      "cb8c42f9029a4b18adcfdee52dbefaf8",
      "79adf09f1c624b74869fc217cd83b5ce",
      "398ed54c272b4b92ac235f486e10283e",
      "c808e3e4a82a454db38ef653ecafb3b6",
      "c7768c44a99a4273bfe9775027a9a06a",
      "12f050066d49421fa3b4eec3e85c5292",
      "9fd182899a864797844f2b822c4fe9aa",
      "516f4517fa99444e8d93c5a257134be8",
      "00e7bdbd5ce24a2fbf6606e86c2f1650",
      "4589a061653341c99218be201026d417",
      "da7ac6a20d8142c4aa331c219aefaca6",
      "ce7b93c6751c446986b123f7c94b18a7",
      "c15d896487514bdc9715044d4064a929",
      "75169e302a6840c4b7c0027b0c0531a7",
      "10e4f80f49a04d1fb1a4ef285f3c1e4e",
      "0fd569c5fc6c4690b9f7890091e00dad",
      "05cbaa0a2f11425d91b320709ef39ad0",
      "0500d3c397dd4928a20edb5a9c8c7edd",
      "1bde65aeeb7a45ce8c00f144a9d88422",
      "0fe465c101ec475d8c8ea690597e2387",
      "4a81af96a727498f9487e822ac619463",
      "e16e3cd6d18b4ed7b2df2e7e06d75662"
     ]
    },
    "id": "GHeoFZrOsAPc",
    "outputId": "724f4bd2-cd7a-4cbb-b109-4637a8c6287d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `Perplexity` from `torchmetrics` was deprecated and will be removed in 2.0. Import `Perplexity` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/usr/local/lib/python3.12/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM_15 | 46.7 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "46.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "46.7 M    Total params\n",
      "186.696   Total estimated model params size (MB)\n",
      "59        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83534e0451749688f419dbadbb3b901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688b298126b14ef3977f6181f513e7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1ade6aa15742409fb417583e0a624b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8c42f9029a4b18adcfdee52dbefaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7b93c6751c446986b123f7c94b18a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_15 = train_method(ModelClass=HybridLM_15,train_size=30000,val_size=5000,n_layer=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kKZ9Vv9hwXl",
    "outputId": "edab599a-337c-4f3f-c96a-1272bae33446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0234 | Perplexity : 1.02\n"
     ]
    }
   ],
   "source": [
    "val_perp_15 = trainer_15.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss_15 = trainer_15.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss_15:.4f} | Perplexity : {val_perp_15:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMu9LNdPop2L"
   },
   "source": [
    "### **Train(16)**\n",
    "\n",
    "model ( 1 Layers ) : ConvBlock\n",
    "\n",
    "train_size : 30000\n",
    "\n",
    "val_size : 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Qx12TU6bop2M"
   },
   "outputs": [],
   "source": [
    "class HybridLM_16(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 1, \"This script assumes 5 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "        ConvBlock(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "     #   MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "      #  MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "     #   TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "e7018d803a4a4d2fbde490416270e375",
      "c505f5dd539544a4a4130506292d83e0",
      "9d571d0b16eb4ef5a01518e14d094aca",
      "c2122efbfc054547a718167a736aca63",
      "80782e0072aa40a58398a7a6c8c280b6",
      "0bf45ff2992e4afbab0e384d3fb97672",
      "f4981bee691c43e99d21aa1c96a85285",
      "6d0a1bfaa4b747df98466fc34e10c89c",
      "9beaa96187554e958eaf037539e51b99",
      "c8c85bcecc7a4f1a94f95eb69a804d3a",
      "1907df3492964d57bc17d38a60241292",
      "8b7ac8726e0b408596cbbdb4deabefe2",
      "7712b77377ea4b19b2c7d1c23058b753",
      "a036e8338aaf4b97a80d89870ce83c2b",
      "0da1eecc32b142a1b66f8e237729e988",
      "6225fa894d2842eea2710ca897ae48f6",
      "b876e9e74f3b4ad2b5967391672b6695",
      "9aeb1e0c74ea48e1932ca52e165f6f4c",
      "afebc6b9e8de497fb44fa4f32bb85ae4",
      "2391f7792ad84e2a825778969beb1f31",
      "6ed07d86c35a4565b47231d5a48dd646",
      "f27640acee6f465391fb5c85d54be8ee",
      "317b4387eba74924ae3ae0c4e7f17d30",
      "d29dc612c2c449b681c2e4992b11c437",
      "707136c23ab1476ba0fc6e2c0cb44c0b",
      "d32c1f8a16eb4dc28281337404836391",
      "fbcb54dec39c4d6dbac380e305be4520",
      "669d0686d8b44c0d910c62b225278604",
      "e14104cc70764099922575f85b6b9e93",
      "5ffe1428eed84e049b19ec11af68af24",
      "3b14123e1cc84d2b8cc34dcfbff7af6e",
      "bfdcad01a364482bb126c43453d55571",
      "dbcb1b771f0b47988606e8bd93aa0f92",
      "34c9bae368294363ab4bbf229f5dc4dc",
      "00b7f299c2394fd89c876fbec3db5615",
      "6341700ca53a4bec9ac0268fa86f5568",
      "d84aaf1e8c034272b127139abb8d87e5",
      "8e753f630b8c4002a95ba9ca8bc8db43",
      "706e57ff6a0544f69b806a8d732bb6d1",
      "3cbda452d47e4301b9a67757d7d3927a",
      "f74d6fa3aa6943fb954da4fde65e401a",
      "fed342c9f5714f70bd9ca7a17434c5f6",
      "6cbc89baafb64ee1b20882eedc0c2524",
      "a1fc7e871fd7404d979c26decf474a39",
      "ba055c946b064bbb924b3f5378db93c8",
      "e88a7f2fcc944fc89dfc4dcbc6a0a24e",
      "9e19f01efe03441e8f7e9c37912d91a0",
      "2beab95122f54b0db0a1a6573813fed9",
      "587068cafd5a48efa7f00fb0798a108d",
      "2d82894403db48aa872d977bffda4f3a",
      "3702ab8501024dffb5d536ed33cf306d",
      "3561990aef064973b6a801f556955385",
      "ac0e16f099c645149b4184e4aa0a00f8",
      "9ea6813524234a5abf26c9c159be8d4b",
      "f38c070fee074e2295415dc82ded9120"
     ]
    },
    "id": "z5jIovAKop2M",
    "outputId": "05e12352-ce65-41a8-dcc0-37365c4c1b9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM_16 | 40.2 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "40.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "40.2 M    Total params\n",
      "160.692   Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7018d803a4a4d2fbde490416270e375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7ac8726e0b408596cbbdb4deabefe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317b4387eba74924ae3ae0c4e7f17d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c9bae368294363ab4bbf229f5dc4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba055c946b064bbb924b3f5378db93c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_16 = train_method(ModelClass=HybridLM_16,train_size=30000,val_size=5000,n_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2A6LcOuqop2N",
    "outputId": "c474aa25-d2aa-471f-e3f5-7848fe3fe720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0221 | Perplexity : 1.02\n"
     ]
    }
   ],
   "source": [
    "val_perp_16 = trainer_16.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss_16 = trainer_16.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss_16:.4f} | Perplexity : {val_perp_16:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90vmSlLPs9oB"
   },
   "source": [
    "### **Train(17)**\n",
    "\n",
    "model ( 1 Layers ) : TransformerBlock\n",
    "\n",
    "train_size : 30000\n",
    "\n",
    "val_size : 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LURgIJR4s9oC"
   },
   "outputs": [],
   "source": [
    "class HybridLM_17(nn.Module):\n",
    "  def __init__(self, vocab_size:int, seq_len:int, n_layer:int, n_head:int, n_embd:int, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "    super().__init__()\n",
    "    assert n_layer == 1, \"This script assumes 5 total layers in a fixed hybrid plan\"\n",
    "    self.seq_len = seq_len\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd) # Added n_embd here\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1,seq_len,n_embd))\n",
    "    self.drop = nn.Dropout(resid_pdrop)\n",
    "    self.layers = nn.ModuleList([\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 1, changed dropout to pdrop\n",
    "      #  ConvBlock(n_embd,pdrop=resid_pdrop), # 2, changed dropout to pdrop\n",
    "        TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 3\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 4\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 5\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 6\n",
    "      #  TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop), # 7\n",
    "     #   MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 8, changed dropout to pdrop\n",
    "      #  MLPBlock(n_embd,mlp_ratio=4.0,pdrop=resid_pdrop), # 9, changed dropout to pdrop\n",
    "     #   TransformerBlock(n_embd,n_head,seq_len,attn_pdrop=attn_pdrop,resid_pdrop=resid_pdrop) # 10\n",
    "    ])\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd,vocab_size,bias=False)\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self,m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "      if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m,nn.Embedding):\n",
    "      nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "\n",
    "  def forward(self,idx,targets=None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.seq_len\n",
    "    x = self.tok_emb(idx) + self.pos_emb[:,:T,:]\n",
    "    x = self.drop(x)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    x = self.ln_f(x)\n",
    "    logits = self.head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
    "    return logits,loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self,idx,max_new_tokens=100,temperature=1.0,top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:,-self.seq_len:]\n",
    "      logits,_ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / max(1e-8,temperature) # Corrected temperature variable name\n",
    "      if top_k is not None :\n",
    "        v,_ = torch.topk(logits,top_k)\n",
    "        logits[logits < v[:,[-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits,dim=-1)\n",
    "      next_id = torch.multinomial(probs,num_samples=1)\n",
    "      idx = torch.cat((idx,next_id),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "b2cf3003baab41ff9aa2a3f222329a24",
      "51f55e8ba9554160a1954b463773344f",
      "f098c324b222429580897613cd662dde",
      "87ab669c774d40408d444df6717f2b55",
      "eceeaedb0a494d20a2fadacc4fd903c9",
      "291071f27a2d42b38bf77946ff333ea3",
      "36e20e7dcc754a7aa8cd207902c569ab",
      "5a4e57a8a8b54c17a2c93f6f083535ec",
      "9f4b0b129e2e41f29f7425130c78143f",
      "6b4e15b955ad4739af19ca89a7328c8e",
      "41fa3cabac17411e8cf8a0f5ffbf01ba",
      "48145742046e42d098e4c45e560dfae0",
      "b26037436f0943ccaf4e700d3048249c",
      "15ffcdab8c594ba79d5b8f683b7a4c0b",
      "6db31890687d4ec9be3c4498584bde59",
      "f28dfc93408e4f018310e93cffca11a0",
      "80ea981129b3458885f25456eef7d312",
      "ef0f20a1396645fd96f4339d01ec59a3",
      "4708ee20740647e49ca1eba6b2cbd1e3",
      "2dc925bdeccd42ad90791c4f15e56c3b",
      "26a5df6a287c4c1c87c4d0e29906c939",
      "ae6ade43fb7f4918b2ef43f5fedf9ae1",
      "03e65da4839142e1bf58f2814856549c",
      "25b3df6eb1024b6182d9a425242c8e98",
      "af293e97b65045229b351a544e842791",
      "f2bcd5854da64299a91e2e981807c32b",
      "a90aa50f59a6411bbbaa251e6564c1f9",
      "40ef1c6fc8b24da8a2b0adaf04168b84",
      "49291546978b4e09928e1902cb5fba85",
      "e0c3f7d5068f4462b5e5f40f60fb3df2",
      "7f7135d5f3db48da872c6b16f4551643",
      "812c72016a0b49698e285920ca2d4eb6",
      "e57cc738af7044c1b6a1b0d1ff0fee26",
      "e1f8a720dc1244929c9f025c943864ac",
      "f9d8c7c5e6d64151bb45f1834a401dcc",
      "d44ff88bade749d5a82c24522dd14466",
      "a5c3f40f51f840edac4c469ceefb742c",
      "e447b51f643a4317aebd944703fac62a",
      "5e59fdce56ba42139c1d66e7e7d4a082",
      "4c70b016f8234d6582c57b0e5c887176",
      "c9e8ebc36d5e41a09ba3d57c489efd91",
      "46c023795275462da5d8d9a891d86317",
      "060e84eb71b44e36996b1303bc9d7e79",
      "dc12ea6f9c004d078e8a73e7925ee1cc",
      "bf9ea07fa39243328b49308cde35a3ea",
      "212904009162474faf06ba05e22e1206",
      "e8696afd7ff04dda99625af19edf4b3d",
      "d6dfaba7da314c47aabaf4c0b5c014de",
      "05c35c1e44df441c9da9554b35800e52",
      "12318f3e223a4bc59cd18dc2b28fd416",
      "afa96b29b42d4e13a8641885e5708f9d",
      "7c1db2855f504ee89461b9dd6f2cf304",
      "54491c87679847939bd43fa755fb7b41",
      "fa16c7d114744fab85c82fb28d82264c",
      "598ffa894c8a447aa6f02419e2c9cc2b"
     ]
    },
    "id": "dzDC4Cwds9oD",
    "outputId": "d18d50d5-30f6-4358-9dec-174f357ee876"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `Perplexity` from `torchmetrics` was deprecated and will be removed in 2.0. Import `Perplexity` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/usr/local/lib/python3.12/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model      | HybridLM_17 | 40.5 M | train\n",
      "1 | perplexity | _Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "40.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "40.5 M    Total params\n",
      "161.878   Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cf3003baab41ff9aa2a3f222329a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48145742046e42d098e4c45e560dfae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e65da4839142e1bf58f2814856549c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f8a720dc1244929c9f025c943864ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9ea07fa39243328b49308cde35a3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_17 = train_method(ModelClass=HybridLM_17,train_size=30000,val_size=5000,n_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiOvOJocs9oD",
    "outputId": "0fbdea95-7a44-4b0e-c839-3f7bff7764e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 1.2972 | Perplexity : 4.45\n"
     ]
    }
   ],
   "source": [
    "val_perp_17 = trainer_17.callback_metrics[\"val_perplexity\"].item()\n",
    "val_loss_17 = trainer_17.callback_metrics[\"val_loss\"].item()\n",
    "print(f\"Validation Loss : {val_loss_17:.4f} | Perplexity : {val_perp_17:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "cells": [],
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.10.12"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
